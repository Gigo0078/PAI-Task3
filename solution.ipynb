{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import warnings\n",
    "from typing import Union\n",
    "from utils import ReplayBuffer, get_env, run_episode\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPISODES = 50\n",
    "TEST_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    This class implements a neural network with a variable number of hidden layers and hidden units.\n",
    "    You may use this function to parametrize your policy and critic networks.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_size: int, \n",
    "                                hidden_layers: int, activation: str):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # TODO: Implement this function which should define a neural network \n",
    "        # with a variable number of hidden layers and hidden units.\n",
    "        # Here you should define layers which your network will use.\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh()\n",
    "            }\n",
    "        #self.activation = self.activations[activation]\n",
    "        self.activation = nn.ReLU()\n",
    "        self.input = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.hidden_size,self.hidden_size) for i in range(self.hidden_layers)])\n",
    "        self.putput = nn.Linear(self.hidden_size, self.output_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement the forward pass for the neural network you have defined.\n",
    "        #pass\n",
    "        s = self.input(s)\n",
    "        s = self.activation(s)\n",
    "        #print(\"after activation\", s)\n",
    "        for i in range(0,self.hidden_layers):\n",
    "            \n",
    "            s = self.linears[i](s)\n",
    "            #print(\"linear layer\", s)\n",
    "\n",
    "            s = self.activation(s)\n",
    "            #print(\"activation in linear layer\", s)\n",
    "            \n",
    "        s = self.putput(s)\n",
    "        #print(\"output is\", s)\n",
    "        #s = self.activation(s)\n",
    "        #print(\"after activation output layer\", s)\n",
    "        #log_s = nn.Softmax(s)\n",
    "        return s #, log_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Actor:\n",
    "    def __init__(self,hidden_size: int, hidden_layers: int, actor_lr: float,\n",
    "                state_dim: int = 3, action_dim: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.actor_lr = actor_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.setup_actor()\n",
    "\n",
    "    def setup_actor(self):\n",
    "        '''\n",
    "        This function sets up the actor network in the Actor class.\n",
    "        '''\n",
    "        # TODO: Implement this function which sets up the actor network. \n",
    "        # Take a look at the NeuralNetwork class in utils.py. \n",
    "        #pass\n",
    "        self.NN_actor = NeuralNetwork(input_dim=self.state_dim, output_dim=2*self.action_dim, hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, activation=\"relu\")\n",
    "        self.NN_actor.to(self.device)\n",
    "        self.optimizer= optim.Adam(self.NN_actor.parameters(),lr = self.actor_lr)\n",
    "        self.temperature = TrainableParameter(init_param=0.005, lr_param=0.1, train_param=True)\n",
    "\n",
    "    def clamp_log_std(self, log_std: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param log_std: torch.Tensor, log_std of the policy.\n",
    "        Returns:\n",
    "        :param log_std: torch.Tensor, log_std of the policy clamped between LOG_STD_MIN and LOG_STD_MAX.\n",
    "        '''\n",
    "        return torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "    def get_action_and_log_prob(self, state: torch.Tensor, \n",
    "                                deterministic: bool = False) -> (torch.Tensor, torch.Tensor):\n",
    "        '''\n",
    "        :param state: torch.Tensor, state of the agent\n",
    "        :param deterministic: boolean, if true return a deterministic action \n",
    "                                otherwise sample from the policy distribution.\n",
    "        Returns:\n",
    "        :param action: torch.Tensor, action the policy returns for the state.\n",
    "        :param log_prob: log_probability of the the action.\n",
    "        '''\n",
    "        \n",
    "        print(\"input state is\", state.shape)\n",
    "        assert state.shape == (3,) or state.shape[1] == self.state_dim, 'State passed to this method has a wrong shape'\n",
    "        action , log_prob = torch.zeros(state.shape[0]), torch.ones(state.shape[0])\n",
    "        # TODO: Implement this function which returns an action and its log probability.\n",
    "        # If working with stochastic policies, make sure that its log_std are clamped \n",
    "        # using the clamp_log_std function.\n",
    "        \n",
    "        #state = torch.tensor(state) # assume state is either [3,] or [200, 3]\n",
    "\n",
    "        #mean, std = torch.chunk(self.NN_actor(state), 2, dim=-1)#.to(self.device)\n",
    "\n",
    "        outputs = self.NN_actor(state)\n",
    "\n",
    "\n",
    "        if state.shape == (3,): # one state\n",
    "            outputs = [outputs]\n",
    "\n",
    "        print(\"outputs\", outputs)\n",
    "        \n",
    "            # mean, std = out # need to check if can unpack\n",
    "\n",
    "            #     std = torch.tensor(torch.abs(std))\n",
    "            #     mean = torch.tensor(mean)\n",
    "\n",
    "            #     log_std = self.clamp_log_std(torch.log(std))   #The log of the standard deviation must be clamped not the standard deviation\n",
    "            #     std = torch.exp(log_std)\n",
    "\n",
    "            #     print(\"mean\", mean)\n",
    "            #     print(\"std\", std)\n",
    "\n",
    "            #     dist = torch.distributions.normal.Normal(mean, std)\n",
    "\n",
    "            #     if deterministic == False:  #We aren't sure about the placement of the clamping, as it makes a difference for the probability, what its std is\n",
    "        \n",
    "            #         action = dist.rsample() #rsample includes the reparametrization trick\n",
    "            #         action = torch.tanh(action)\n",
    "\n",
    "            #     else:\n",
    "            #         action = mean\n",
    "\n",
    "            #     log_prob = dist.log_prob(action)\n",
    "\n",
    "        actions, log_probs = [], []\n",
    "\n",
    "        for out in outputs:\n",
    "\n",
    "            print(\"out\", out)\n",
    "\n",
    "            mean, std = out # need to check if can unpack\n",
    "\n",
    "            std = torch.tensor(torch.abs(std))\n",
    "            mean = torch.tensor(mean)\n",
    "\n",
    "            log_std = self.clamp_log_std(torch.log(std))   #The log of the standard deviation must be clamped not the standard deviation\n",
    "            std = torch.exp(log_std)\n",
    "\n",
    "            print(\"mean\", mean)\n",
    "            print(\"std\", std)\n",
    "\n",
    "            dist = torch.distributions.normal.Normal(mean, std)\n",
    "\n",
    "            if deterministic == False:  #We aren't sure about the placement of the clamping, as it makes a difference for the probability, what its std is\n",
    "        \n",
    "                action = dist.rsample() #rsample includes the reparametrization trick\n",
    "                action = torch.tanh(action)\n",
    "\n",
    "            else:\n",
    "                action = mean\n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "            actions.append(action) \n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "        print(\"actions\", actions)\n",
    "        print(\"log_probs\", log_probs)\n",
    "\n",
    "        if state.shape[0] == self.state_dim: # working with a single state\n",
    "            \n",
    "            actions = torch.tensor(actions).reshape((self.action_dim, ))\n",
    "            log_probs = torch.tensor(log_probs).reshape((self.action_dim, ))\n",
    "\n",
    "        else:  # second dimension is the state shape\n",
    "\n",
    "            N = state.shape[0]\n",
    "            actions = torch.tensor(actions).reshape((N, self.action_dim))\n",
    "            log_probs = torch.tensor(log_probs).reshape((N, self.action_dim))\n",
    "\n",
    "\n",
    "        print(\"-------after conversion to tensors\")\n",
    "        print(\"actions\", actions)\n",
    "        print(\"log_probs\", log_probs)\n",
    "\n",
    "        assert (actions.shape == (self.action_dim, ) and \\\n",
    "            log_probs.shape == (self.action_dim, ), 'Incorrect shape for action or log_prob.' ) or \\\n",
    "                ( actions.shape[1] == self.action_dim and log_probs.shape[1] == self.action_dim )\n",
    "             \n",
    "        return actions, log_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "torch.Size([7, 3])\n",
      "input state is torch.Size([7, 3])\n",
      "outputs tensor([[-0.0541, -0.0497],\n",
      "        [-0.0562, -0.0474],\n",
      "        [-0.0815, -0.0309],\n",
      "        [-0.0638, -0.0482],\n",
      "        [-0.0670, -0.0455],\n",
      "        [-0.0638, -0.0482],\n",
      "        [-0.0638, -0.0482]], grad_fn=<AddmmBackward0>)\n",
      "out tensor([-0.0541, -0.0497], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0541)\n",
      "std tensor(0.0497)\n",
      "out tensor([-0.0562, -0.0474], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0562)\n",
      "std tensor(0.0474)\n",
      "out tensor([-0.0815, -0.0309], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0815)\n",
      "std tensor(0.0309)\n",
      "out tensor([-0.0638, -0.0482], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0638)\n",
      "std tensor(0.0482)\n",
      "out tensor([-0.0670, -0.0455], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0670)\n",
      "std tensor(0.0455)\n",
      "out tensor([-0.0638, -0.0482], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0638)\n",
      "std tensor(0.0482)\n",
      "out tensor([-0.0638, -0.0482], grad_fn=<UnbindBackward0>)\n",
      "mean tensor(-0.0638)\n",
      "std tensor(0.0482)\n",
      "actions [tensor(-0.0178), tensor(0.0320), tensor(-0.0869), tensor(-0.0379), tensor(-0.0181), tensor(-0.1669), tensor(-0.0492)]\n",
      "log_probs [tensor(1.8175), tensor(0.3974), tensor(2.5429), tensor(1.9690), tensor(1.5929), tensor(-0.1773), tensor(2.0680)]\n",
      "-------after conversion to tensors\n",
      "actions tensor([[-0.0178],\n",
      "        [ 0.0320],\n",
      "        [-0.0869],\n",
      "        [-0.0379],\n",
      "        [-0.0181],\n",
      "        [-0.1669],\n",
      "        [-0.0492]])\n",
      "log_probs tensor([[ 1.8175],\n",
      "        [ 0.3974],\n",
      "        [ 2.5429],\n",
      "        [ 1.9690],\n",
      "        [ 1.5929],\n",
      "        [-0.1773],\n",
      "        [ 2.0680]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/2547pfrs2tv5kkvqzk0wydg00000gn/T/ipykernel_78071/3006107207.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(torch.abs(std))\n",
      "/var/folders/ss/2547pfrs2tv5kkvqzk0wydg00000gn/T/ipykernel_78071/3006107207.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0178],\n",
       "         [ 0.0320],\n",
       "         [-0.0869],\n",
       "         [-0.0379],\n",
       "         [-0.0181],\n",
       "         [-0.1669],\n",
       "         [-0.0492]]),\n",
       " tensor([[ 1.8175],\n",
       "         [ 0.3974],\n",
       "         [ 2.5429],\n",
       "         [ 1.9690],\n",
       "         [ 1.5929],\n",
       "         [-0.1773],\n",
       "         [ 2.0680]]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# testing forward pass\n",
    "\n",
    "input_tensor = torch.tensor([[-0.9985, -0.0543,  0.4931],\n",
    "        [-0.9988, -0.0494,  0.3297],\n",
    "        [-0.9996, -0.0295, -0.4181],\n",
    "        [-0.9924,  0.1230, -0.0246],\n",
    "        [-0.9926,  0.1210, -0.0899],\n",
    "        [-0.9924,  0.1230, -0.0246],\n",
    "        [-0.9924,  0.1230, -0.0246]])\n",
    "\n",
    "#input_tensor = torch.tensor([[-0.9985,-0.0543,0.4931]]).reshape((3,))\n",
    "\n",
    "print(input_tensor.shape)\n",
    "\n",
    "\n",
    "agent.actor.get_action_and_log_prob(input_tensor, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, hidden_size: int, \n",
    "                 hidden_layers: int, critic_lr: int, state_dim: int = 3, \n",
    "                    action_dim: int = 1,device: torch.device = torch.device('cpu')):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.critic_lr = critic_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.setup_critic()\n",
    "\n",
    "    def setup_critic(self):\n",
    "        # TODO: Implement this function which sets up the critic(s). Take a look at the NeuralNetwork \n",
    "        # class in utils.py. Note that you can have MULTIPLE critic networks in this class.\n",
    "\n",
    "        self.NN_critic = NeuralNetwork(input_dim = self.state_dim, output_dim=1, hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, activation=\"relu\")\n",
    "        self.NN_critic.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.NN_critic.parameters(),lr = self.critic_lr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableParameter:\n",
    "    '''\n",
    "    This class could be used to define a trainable parameter in your method. You could find it \n",
    "    useful if you try to implement the entropy temerature parameter for SAC algorithm.\n",
    "    '''\n",
    "    def __init__(self, init_param: float, lr_param: float, \n",
    "                 train_param: bool, device: torch.device = torch.device('cpu')):\n",
    "        \n",
    "        self.log_param = torch.tensor(np.log(init_param), requires_grad=train_param, device=device)\n",
    "        self.optimizer = optim.Adam([self.log_param], lr=lr_param)\n",
    "\n",
    "    def get_param(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_param)\n",
    "\n",
    "    def get_log_param(self) -> torch.Tensor:\n",
    "        return self.log_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Environment variables. You don't need to change this.\n",
    "        self.state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "        self.action_dim = 1  # [torque] in[-1,1]\n",
    "        self.batch_size = 200\n",
    "        self.min_buffer_size = 1000\n",
    "        self.max_buffer_size = 100000\n",
    "        # If your PC possesses a GPU, you should be able to use it for training, \n",
    "        # as self.device should be 'cuda' in that case.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device: {}\".format(self.device))\n",
    "        self.memory = ReplayBuffer(self.min_buffer_size, self.max_buffer_size, self.device)\n",
    "        \n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        # TODO: Setup off-policy agent with policy and critic classes. \n",
    "        # Feel free to instantiate any other parameters you feel you might need.   \n",
    "        #pass\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_size = 256\n",
    "        self.lr = 3E-3\n",
    "\n",
    "        self.actor = Actor(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        self.critic_Q2 = Critic(state_dim=self.state_dim+self.action_dim,\n",
    "                                hidden_size=self.hidden_size, \n",
    "                                hidden_layers=self.hidden_layers,\n",
    "                                critic_lr=self.lr)\n",
    "        \n",
    "        self.critic_Q1 = Critic(state_dim=self.state_dim+self.action_dim,\n",
    "                                hidden_size=self.hidden_size,\n",
    "                                hidden_layers=self.hidden_layers,\n",
    "                                critic_lr=self.lr)\n",
    "        #self.critic = Critic(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        #self.trainable_params = TrainableParameter(init_param: float, self.lr, train_param: bool)\n",
    "        #Name parameters from the paper\n",
    "        #self.log_prob = []\n",
    "        self.Tau = 0.005\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def get_action(self, s: np.ndarray, train: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param s: np.ndarray, state of the pendulum. shape (3, )\n",
    "        :param train: boolean to indicate if you are in eval or train mode. \n",
    "                    You can find it useful if you want to sample from deterministic policy.\n",
    "        :return: np.ndarray,, action to apply on the environment, shape (1,)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a function that returns an action from the policy for the state s.\n",
    "        #action = np.random.uniform(-1, 1, (1,))\n",
    "        #Convert the state to a torch tensor, which is the required input for the actor\n",
    "        s = torch.tensor(s)\n",
    "        #Import action from the actor and discard the log probability here, possibly used elsewhere\n",
    "        action, _ = self.actor.get_action_and_log_prob(s, False)\n",
    "        # only get one action -> we have to sample in get_action_and_log_prob\n",
    "        #Convert the returned tensor action to an nd.array\n",
    "        action = action.clone().detach().numpy()\n",
    "        #Need log probability for something -------> ?\n",
    "\n",
    "        assert action.shape == (1,), 'Incorrect action shape.'\n",
    "        assert isinstance(action, np.ndarray ), 'Action dtype must be np.ndarray' \n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    # loss: 200 x 1\n",
    "    def run_gradient_update_step(object: Union[Actor, Critic], loss: torch.Tensor):\n",
    "        '''\n",
    "        This function takes in a object containing trainable parameters and an optimizer, \n",
    "        and using a given loss, runs one step of gradient update. If you set up trainable parameters \n",
    "        and optimizer inside the object, you could find this function useful while training.\n",
    "        :param object: object containing trainable parameters and an optimizer\n",
    "        '''\n",
    "        object.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        object.optimizer.step()\n",
    "\n",
    "    def critic_target_update(self, base_net: NeuralNetwork, target_net: NeuralNetwork, \n",
    "                             tau: float, soft_update: bool):\n",
    "        '''\n",
    "        This method updates the target network parameters using the source network parameters.\n",
    "        If soft_update is True, then perform a soft update, otherwise a hard update (copy).\n",
    "        :param base_net: source network\n",
    "        :param target_net: target network\n",
    "        :param tau: soft update parameter\n",
    "        :param soft_update: boolean to indicate whether to perform a soft update or not\n",
    "        '''\n",
    "        for param_target, param in zip(target_net.parameters(), base_net.parameters()):\n",
    "            if soft_update:\n",
    "                param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "            else:\n",
    "                param_target.data.copy_(param.data)\n",
    "\n",
    "    def train_agent(self): \n",
    "        '''\n",
    "        This function represents one training iteration for the agent. It samples a batch \n",
    "        from the replay buffer,and then updates the policy and critic networks \n",
    "        using the sampled batch.\n",
    "        '''\n",
    "        # TODO: Implement one step of training for the agent.\n",
    "        # Hint: You can use the run_gradient_update_step for each policy and critic.\n",
    "        # Example: self.run_gradient_update_step(self.policy, policy_loss)\n",
    "        # Batch sampling\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch = batch\n",
    "\n",
    "        print(\"#############################\")\n",
    "        print(\"train_agent\")\n",
    "        print(\"#############################\")\n",
    "\n",
    "        #Get the temperature - We still need to figure out which network uses this\n",
    "        alpha = self.actor.temperature.get_param()\n",
    "        print(\"alpha before optimization\", alpha)\n",
    "        #alpha = torch.tensor(0.5)\n",
    "        reward =  1/alpha * r_batch # smth to investigate\n",
    "        print(\"modified reward\", reward[0:5, :])\n",
    "        #reward = r_batch + alpha * entropy <--\n",
    "\n",
    "        #Store the basic Psi network - which I guess we still need\n",
    "        base_net1 = NeuralNetwork(input_dim = self.state_dim + self.action_dim, \n",
    "                                  output_dim = 1, \n",
    "                                  hidden_size = 256,\n",
    "                                  hidden_layers = 2,\n",
    "                                  activation=\"relu\").to(self.device) #self.critic_Q1.NN_critic #self.critic_Q1.NN_critic\n",
    "        \n",
    "        base_net2 = NeuralNetwork(input_dim = self.state_dim + self.action_dim, \n",
    "                                  output_dim = 1, \n",
    "                                  hidden_size = 256,\n",
    "                                  hidden_layers = 2,\n",
    "                                  activation=\"relu\").to(self.device) #self.critic_Q2.NN_critic\n",
    "\n",
    "        base_net1.load_state_dict(copy.deepcopy(self.critic_Q1.NN_critic.state_dict()))\n",
    "        base_net1.to(self.device)\n",
    "        base_net2.load_state_dict(copy.deepcopy(self.critic_Q2.NN_critic.state_dict()))\n",
    "        base_net2.to(self.device)\n",
    "\n",
    "        print(\"Q1 before gradient\", base_net1.state_dict()['putput.weight'][0,:5])\n",
    "\n",
    "        #Optimize the critic networks\n",
    "        #Run a gradient update step for critic V\n",
    "        # TODO: Implement Critic(s) update here.\n",
    "\n",
    "       \n",
    "        with torch.no_grad():\n",
    "\n",
    "            results_list = [self.actor.get_action_and_log_prob(state, False) for state in s_prime_batch] \n",
    "            \n",
    "            next_sampled_action, next_sampled_log_prob = zip(*results_list)\n",
    "\n",
    "            next_sampled_action = torch.tensor(next_sampled_action).flatten().reshape(self.batch_size, 1)\n",
    "            next_sampled_log_prob = torch.tensor(next_sampled_log_prob).flatten().reshape(self.batch_size, 1)\n",
    "\n",
    "            print(\"next_sampled_action\",next_sampled_action[0:5,:])\n",
    "            print(\"next_sampled_log_prob\", next_sampled_log_prob[0:5,:])\n",
    "\n",
    "            input = torch.cat((s_prime_batch, next_sampled_action), dim = 1).to(self.device)\n",
    "            print(\"input looks like\", input[0:5,])\n",
    "\n",
    "            qf1_next = self.critic_Q1.NN_critic(input)   \n",
    "            qf2_next = self.critic_Q2.NN_critic(input)\n",
    "\n",
    "\n",
    "\n",
    "            print(\"Total number of zero outputs\", (qf1_next == 0).sum(), \"out of\", qf1_next.shape)\n",
    "\n",
    "            min_qf_next = torch.min(qf1_next,qf2_next) - next_sampled_log_prob\n",
    "\n",
    "            print(\"min_qf_next\",min_qf_next[0:5,:])\n",
    "\n",
    "            next_q_value = reward + self.gamma * min_qf_next # 200 x 1\n",
    "\n",
    "        print(\"next_q_value\", next_q_value[0:5,:])\n",
    "\n",
    "        #Get the current values and optimize with respect to the next ones\n",
    "        input_Q = torch.cat((s_batch, a_batch), dim = 1).to(self.device)\n",
    "    \n",
    "        qf1 = self.critic_Q1.NN_critic(input_Q) # 200 x 1\n",
    "        qf2 = self.critic_Q2.NN_critic(input_Q) # 200 x 1\n",
    "\n",
    "        print(\"Total number of zero outputs\", (qf1 == 0).sum(), \"out of\", qf1.shape)\n",
    "\n",
    "        q1_loss = nn.functional.mse_loss(qf1, next_q_value)  \n",
    "        q2_loss = nn.functional.mse_loss(qf2,next_q_value)\n",
    "\n",
    "        print(\"q1 loss\", q1_loss)\n",
    "\n",
    "        self.run_gradient_update_step(self.critic_Q1, q1_loss)\n",
    "        self.run_gradient_update_step(self.critic_Q2, q2_loss)\n",
    "\n",
    "        # print some gradients\n",
    "\n",
    "        print(\"grad of NN_critic Q1 putput weight\", self.critic_Q1.NN_critic.putput.weight.grad)\n",
    "        print(\"grad of NN_critic Q2 putput weight\", self.critic_Q2.NN_critic.putput.weight.grad)\n",
    "        #print(\"grad of NN_critic Q2\", )\n",
    "\n",
    "        #Sample current action and its log_prob\n",
    "        with torch.no_grad():\n",
    "            results_list2 = [self.actor.get_action_and_log_prob(state, False) for state in s_batch]\n",
    "\n",
    "            sampled_action, sampled_log_prob = zip(*results_list2) #self.actor.get_action_and_log_prob(state=s_batch, deterministic=False)\n",
    "        \n",
    "            sampled_action = torch.tensor(sampled_action).flatten().reshape(self.batch_size, 1)\n",
    "            sampled_log_prob = torch.tensor(sampled_log_prob).flatten().reshape(self.batch_size, 1)\n",
    "\n",
    "        input_policy = torch.cat((s_batch, sampled_action), dim = 1).to(self.device)\n",
    "        Q1_pi = self.critic_Q1.NN_critic(input_policy) #s_batch,sampled_action)\n",
    "        Q2_pi = self.critic_Q2.NN_critic(input_policy) #s_batch,sampled_action)\n",
    "        min_q_pi = torch.min(Q1_pi, Q2_pi)\n",
    "        \n",
    "        #Policy loss\n",
    "\n",
    "\n",
    "        # TODO: Implement Policy update here\n",
    "        policy_loss = ((sampled_log_prob) - min_q_pi) # self.alpha * removed\n",
    "        print(\"policy loss\", policy_loss[0:5,])\n",
    "\n",
    "        #Gradient update for policy\n",
    "        self.run_gradient_update_step(self.actor, policy_loss)\n",
    "\n",
    "        print(\"grad of policy network\", self.actor.NN_actor.input.weight.grad)\n",
    "\n",
    "\n",
    "        # print some gradients\n",
    "\n",
    "\n",
    "        # Temperature (alpha) loss\n",
    "        print(\"------ Training temperature -------\")\n",
    "\n",
    "        H = -1.\n",
    "        alpha_loss = - alpha * sampled_log_prob - alpha * H\n",
    "\n",
    "        self.actor.temperature.optimizer.zero_grad()\n",
    "        alpha_loss.mean().backward()\n",
    "        self.actor.temperature.optimizer.step()\n",
    "        \n",
    "        print(\"targetnet --> Q1 after gradient, before soft update\", self.critic_Q1.NN_critic.state_dict()['putput.weight'][0,:5])\n",
    "        #Critic target update step\n",
    "        print(\"basenet1 -->\", base_net1.state_dict()['putput.weight'][0,:5])\n",
    "\n",
    "        self.critic_target_update(base_net1, self.critic_Q1.NN_critic, self.Tau,True)\n",
    "        self.critic_target_update(base_net2, self.critic_Q2.NN_critic, self.Tau,True)\n",
    "\n",
    "        print(\"Q1 after update\", self.critic_Q1.NN_critic.state_dict()['putput.weight'][0,:5])\n",
    "\n",
    "        alpha = self.actor.temperature.get_param()\n",
    "        print(\"alpha after optimization\", alpha)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "tensor([[-0.0683, -0.0202],\n",
      "        [-0.0764, -0.0191],\n",
      "        [-0.0667, -0.0296],\n",
      "        [-0.0714, -0.0252],\n",
      "        [-0.0694, -0.0275]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 5.7183e-01, -3.3239e-01, -3.7381e-01],\n",
      "        [-1.1470e-02, -3.5208e-03, -5.6288e-01],\n",
      "        [ 3.6049e-01,  9.2758e-02,  4.6839e-01],\n",
      "        [ 4.4821e-01, -4.6854e-01, -5.5328e-01],\n",
      "        [-2.3429e-01, -5.3039e-01,  1.5475e-01],\n",
      "        [ 2.2095e-01, -2.7516e-01, -4.2512e-01],\n",
      "        [-2.1257e-01, -2.8474e-01,  3.6621e-01],\n",
      "        [-8.9758e-02, -3.7507e-01,  5.1281e-01],\n",
      "        [-1.8807e-01, -6.5548e-02, -3.5136e-01],\n",
      "        [-3.4773e-01, -1.3617e-01,  1.7860e-01],\n",
      "        [-1.3889e-01, -5.1517e-01, -1.7246e-01],\n",
      "        [-1.1834e-01, -1.4372e-01,  3.8751e-01],\n",
      "        [-1.2922e-02, -2.1914e-01, -9.9491e-03],\n",
      "        [ 2.1823e-01, -2.6642e-02,  2.1151e-03],\n",
      "        [ 8.8240e-02, -2.1016e-01, -5.7161e-01],\n",
      "        [-3.2454e-01, -2.9064e-01, -3.2408e-01],\n",
      "        [ 2.0516e-01, -3.5161e-01,  2.3171e-01],\n",
      "        [-1.5970e-01,  3.3371e-01, -2.2288e-01],\n",
      "        [-3.8275e-01, -3.3882e-01,  1.2099e-01],\n",
      "        [-2.6291e-01, -1.1967e-02, -1.9811e-01],\n",
      "        [ 5.0442e-01, -5.4046e-01,  5.6156e-01],\n",
      "        [-3.1499e-01, -8.7766e-02, -4.1801e-01],\n",
      "        [-4.6582e-02,  5.6450e-01, -2.4874e-01],\n",
      "        [ 2.1205e-01,  7.1177e-02,  2.6980e-01],\n",
      "        [ 3.1190e-01,  3.8007e-01, -2.0747e-01],\n",
      "        [-1.8996e-01, -1.2367e-01, -2.0704e-01],\n",
      "        [ 4.9875e-01, -1.7464e-01,  5.4486e-01],\n",
      "        [ 2.1847e-01, -4.6091e-02, -8.8202e-02],\n",
      "        [ 7.7620e-02,  4.7725e-01,  2.4253e-01],\n",
      "        [ 4.6838e-01, -5.2754e-01, -2.4343e-01],\n",
      "        [-4.4968e-01,  1.0170e-01,  5.3233e-02],\n",
      "        [-2.8330e-02,  4.9959e-01,  3.3883e-01],\n",
      "        [-5.3325e-01, -3.0640e-03,  1.1672e-01],\n",
      "        [-5.0390e-02, -2.5706e-01, -3.5562e-01],\n",
      "        [-3.7200e-01, -3.4547e-01,  3.1470e-01],\n",
      "        [-5.1334e-01,  2.7830e-01,  1.4065e-02],\n",
      "        [-3.3590e-01,  3.1074e-01,  5.6470e-01],\n",
      "        [-9.4692e-02,  6.6548e-02, -7.6002e-02],\n",
      "        [-1.0769e-01,  1.5905e-01, -1.4799e-01],\n",
      "        [-2.7366e-01, -3.9047e-01, -2.2233e-01],\n",
      "        [ 2.3618e-01, -3.9439e-01, -2.4318e-01],\n",
      "        [-1.9293e-01,  4.5713e-02, -9.2811e-02],\n",
      "        [-5.7564e-01, -1.4168e-01,  3.7958e-01],\n",
      "        [-3.9635e-01, -3.9626e-01,  3.5653e-01],\n",
      "        [-4.8285e-01,  3.3533e-01, -4.6553e-01],\n",
      "        [-4.7962e-01, -4.2659e-01,  3.6209e-01],\n",
      "        [ 5.0527e-01,  8.3659e-02, -5.0614e-02],\n",
      "        [ 1.5701e-01,  1.1915e-01, -2.0638e-02],\n",
      "        [-2.3968e-01,  1.7498e-01,  3.0707e-01],\n",
      "        [ 4.0058e-01, -5.7188e-01,  3.3198e-01],\n",
      "        [-2.0914e-01,  2.3807e-01,  4.6073e-01],\n",
      "        [ 3.8678e-01, -3.9524e-01,  6.6381e-02],\n",
      "        [ 4.0009e-01, -4.7941e-01, -3.4603e-01],\n",
      "        [ 8.9486e-02,  1.2984e-01, -2.8053e-02],\n",
      "        [-4.6709e-01,  1.3675e-01, -4.1239e-01],\n",
      "        [ 1.9655e-01,  1.9705e-01,  4.6405e-01],\n",
      "        [-1.3496e-02,  1.3228e-01,  3.5102e-01],\n",
      "        [-4.0327e-02, -2.6716e-02,  1.3353e-01],\n",
      "        [-3.6827e-01,  4.4813e-01,  4.5475e-01],\n",
      "        [ 2.6647e-01, -1.5492e-01, -5.4621e-02],\n",
      "        [ 5.4585e-01, -4.4191e-01,  1.8919e-01],\n",
      "        [ 2.3837e-01,  4.0067e-01,  3.3030e-02],\n",
      "        [ 1.6156e-02,  2.3030e-02, -9.5875e-02],\n",
      "        [ 3.3058e-01,  4.7571e-01,  5.5478e-01],\n",
      "        [ 2.7606e-01,  3.8582e-01,  1.5825e-01],\n",
      "        [ 2.0840e-02, -2.8145e-01,  5.7390e-01],\n",
      "        [-4.7125e-01,  4.5867e-01, -1.0334e-02],\n",
      "        [ 7.8113e-02,  4.8585e-01,  3.7579e-01],\n",
      "        [-4.7888e-01, -1.9937e-02,  3.3966e-02],\n",
      "        [ 1.5110e-01, -4.2296e-01,  3.0164e-01],\n",
      "        [-2.4326e-01,  1.8295e-01,  3.5316e-01],\n",
      "        [-2.0248e-01, -5.1724e-01, -4.5478e-01],\n",
      "        [ 4.8718e-01,  5.2319e-01, -5.7658e-01],\n",
      "        [-2.7696e-01, -1.0126e-01, -2.3417e-01],\n",
      "        [ 3.9815e-01, -2.7913e-01,  1.2773e-01],\n",
      "        [-2.1719e-01, -2.4315e-02,  1.9685e-01],\n",
      "        [ 2.1405e-01,  5.6312e-01,  5.3513e-01],\n",
      "        [-2.0243e-01,  5.3769e-01, -2.9521e-01],\n",
      "        [-1.5738e-01, -5.0031e-01,  1.2385e-01],\n",
      "        [-8.6744e-02, -3.2437e-01,  1.0586e-01],\n",
      "        [-1.5127e-01,  2.8226e-01, -7.3229e-02],\n",
      "        [-2.2413e-01, -3.2828e-01,  4.1595e-01],\n",
      "        [ 2.7373e-01,  8.9842e-04,  2.2345e-01],\n",
      "        [-1.2372e-01, -5.4351e-01, -3.5141e-01],\n",
      "        [ 1.3952e-01,  5.4542e-01,  1.8643e-03],\n",
      "        [-1.6432e-01,  1.2913e-01,  1.4501e-01],\n",
      "        [ 4.9805e-01, -1.4776e-02,  2.0497e-01],\n",
      "        [ 4.7775e-01, -5.5742e-01, -2.4237e-01],\n",
      "        [ 4.7556e-01,  5.0249e-01,  5.6862e-01],\n",
      "        [ 2.3289e-01, -2.3423e-01, -3.1253e-01],\n",
      "        [ 7.6635e-02,  4.2497e-01, -3.2962e-01],\n",
      "        [ 4.2307e-01, -5.6210e-03, -4.9022e-01],\n",
      "        [-1.2463e-01, -9.4295e-02, -7.4545e-02],\n",
      "        [-3.7727e-01,  4.6627e-01,  4.0411e-01],\n",
      "        [ 4.9717e-01, -5.9546e-02,  2.9262e-01],\n",
      "        [ 7.1923e-02,  1.3074e-01,  7.8907e-02],\n",
      "        [ 4.4694e-02, -1.8650e-01, -3.2795e-03],\n",
      "        [-5.4192e-01, -4.8587e-02, -2.4116e-01],\n",
      "        [-1.9903e-01, -2.0163e-02,  2.3139e-01],\n",
      "        [-2.3826e-02, -3.7821e-01, -5.7702e-01],\n",
      "        [-1.8561e-01,  1.2983e-01, -1.0048e-02],\n",
      "        [ 4.1377e-01, -3.9568e-01, -3.8602e-03],\n",
      "        [ 2.7315e-01,  4.4474e-01,  1.5475e-01],\n",
      "        [-5.5067e-01, -3.2051e-01,  2.6299e-01],\n",
      "        [ 2.2165e-01, -4.3863e-01,  2.2157e-01],\n",
      "        [ 4.5331e-01, -5.5536e-01,  4.3528e-01],\n",
      "        [-4.4567e-01,  4.6229e-01,  3.8011e-01],\n",
      "        [ 4.3624e-01, -1.0647e-01, -4.8354e-01],\n",
      "        [-3.9893e-01, -1.4683e-01, -5.3884e-01],\n",
      "        [ 9.6540e-02, -5.4500e-01, -3.5897e-01],\n",
      "        [-2.2732e-01,  3.4810e-01,  3.5471e-01],\n",
      "        [-2.6132e-01,  4.5704e-01,  1.2022e-01],\n",
      "        [ 4.3211e-02,  5.7542e-01, -1.6980e-01],\n",
      "        [-3.7182e-01,  3.6623e-01,  2.0673e-01],\n",
      "        [-5.9231e-02, -1.8258e-01,  3.5724e-01],\n",
      "        [ 9.2600e-02,  3.8979e-01,  1.8364e-01],\n",
      "        [ 5.5284e-01,  3.8556e-01, -5.8575e-02],\n",
      "        [ 4.3402e-01,  4.1304e-01, -4.3902e-01],\n",
      "        [ 3.1629e-01,  5.5903e-01,  3.5679e-01],\n",
      "        [-1.8704e-01,  3.4051e-02, -4.1501e-01],\n",
      "        [ 1.7944e-01, -4.5083e-01, -6.4377e-02],\n",
      "        [-2.9305e-01,  2.6818e-01, -2.7912e-01],\n",
      "        [ 2.2414e-01, -4.0832e-01,  2.8100e-01],\n",
      "        [-5.0768e-01,  6.2096e-02,  5.0272e-01],\n",
      "        [ 1.6443e-01,  8.0424e-02,  4.5696e-01],\n",
      "        [-2.6039e-01, -4.3943e-01, -2.4234e-01],\n",
      "        [-4.3085e-01, -9.3432e-02,  3.9537e-01],\n",
      "        [ 1.1099e-01, -3.3948e-01,  2.9090e-01],\n",
      "        [ 5.2795e-01,  4.2485e-01, -9.0192e-02],\n",
      "        [ 1.0103e-02,  1.6985e-01, -4.6265e-01],\n",
      "        [ 1.9298e-01, -3.2068e-01,  1.2095e-01],\n",
      "        [ 3.3564e-02,  5.3628e-01,  1.1897e-01],\n",
      "        [ 4.0814e-01, -1.8955e-01, -1.3773e-01],\n",
      "        [ 2.8552e-01, -2.3199e-01,  3.6169e-01],\n",
      "        [ 5.2421e-01,  4.8755e-01, -9.7503e-02],\n",
      "        [ 2.3036e-01, -3.5088e-01,  1.4327e-01],\n",
      "        [-3.7453e-01, -4.1122e-01, -4.0666e-02],\n",
      "        [-1.9695e-01,  6.8486e-02,  4.4523e-01],\n",
      "        [ 5.0036e-01,  1.4607e-01,  1.2734e-01],\n",
      "        [ 4.3870e-01,  1.8986e-01,  1.1317e-01],\n",
      "        [-1.8476e-01, -4.3816e-01,  3.6557e-02],\n",
      "        [-1.4608e-02, -1.9445e-01, -1.7252e-01],\n",
      "        [ 4.5824e-01, -4.3802e-01, -1.2893e-01],\n",
      "        [ 3.7209e-01, -3.3861e-01, -5.0126e-01],\n",
      "        [ 1.0718e-01, -7.0748e-02, -4.3734e-01],\n",
      "        [ 1.9182e-01, -2.2086e-01, -5.3460e-02],\n",
      "        [ 5.3260e-01,  1.7307e-01,  6.6833e-02],\n",
      "        [ 6.8609e-02,  1.1999e-01,  4.5069e-01],\n",
      "        [-3.5853e-01,  3.4135e-01,  2.0726e-01],\n",
      "        [-3.7720e-01, -2.9237e-01, -4.5393e-01],\n",
      "        [ 1.7180e-02, -5.4822e-01, -1.7700e-01],\n",
      "        [-3.2629e-01,  4.0760e-01, -1.7926e-01],\n",
      "        [-3.5770e-01,  4.6673e-01,  7.4387e-02],\n",
      "        [-1.8573e-01, -6.5913e-03, -5.2781e-02],\n",
      "        [-3.0277e-01,  1.4128e-01,  5.0625e-01],\n",
      "        [-4.2428e-01,  5.1200e-01,  9.5466e-02],\n",
      "        [ 5.7232e-01,  5.7616e-01, -1.0663e-01],\n",
      "        [-4.3672e-01, -6.7456e-02, -3.4973e-01],\n",
      "        [ 4.5258e-02, -5.7649e-01, -5.3213e-01],\n",
      "        [-4.3924e-03, -1.0007e-01,  5.6709e-01],\n",
      "        [-5.0563e-01,  2.5593e-01, -1.1727e-01],\n",
      "        [-2.0604e-01, -3.3978e-01,  1.9902e-02],\n",
      "        [-2.1559e-01,  2.8720e-01, -1.7826e-01],\n",
      "        [-3.1072e-01,  2.7136e-01, -2.9303e-01],\n",
      "        [ 2.6498e-01, -1.2927e-01,  5.0086e-01],\n",
      "        [-5.1427e-02,  5.1594e-01, -1.3356e-01],\n",
      "        [-5.5303e-01,  1.1203e-01,  4.5338e-01],\n",
      "        [-1.8687e-01,  7.2426e-04,  4.9877e-01],\n",
      "        [-2.1617e-01, -6.6863e-02,  5.1234e-01],\n",
      "        [-4.8200e-01,  6.9763e-02, -1.8550e-01],\n",
      "        [ 2.6777e-01,  3.5596e-01, -5.6034e-01],\n",
      "        [ 1.5515e-01, -8.6294e-02, -2.6268e-01],\n",
      "        [ 5.5964e-02, -5.2337e-01, -1.6774e-01],\n",
      "        [-1.4518e-01, -2.4974e-02,  1.2093e-01],\n",
      "        [-4.9133e-01, -3.7306e-01,  2.5853e-01],\n",
      "        [-4.6015e-01, -4.7764e-01, -4.8183e-01],\n",
      "        [-3.4085e-01,  2.0283e-01, -1.6046e-01],\n",
      "        [-5.7185e-02,  8.3346e-02, -5.3068e-01],\n",
      "        [ 1.7217e-01, -3.9033e-01,  5.3650e-02],\n",
      "        [ 3.1163e-01,  5.7666e-01,  1.9078e-01],\n",
      "        [-3.5048e-01, -5.6872e-01, -4.7836e-01],\n",
      "        [ 4.5821e-01, -4.7703e-01, -4.5007e-01],\n",
      "        [ 1.2164e-02,  5.0154e-01,  2.2962e-01],\n",
      "        [-3.7033e-02, -5.2981e-01,  6.6716e-02],\n",
      "        [ 6.9376e-03,  2.9283e-01, -3.2546e-01],\n",
      "        [ 1.2315e-01,  3.0159e-02, -1.3589e-01],\n",
      "        [-4.7074e-01,  3.2461e-01,  1.1619e-01],\n",
      "        [-1.7044e-01, -8.0909e-03,  5.6967e-01],\n",
      "        [ 3.5572e-01, -2.2353e-01, -6.2000e-02],\n",
      "        [-1.5269e-02, -5.1569e-01, -5.5915e-01],\n",
      "        [ 4.4900e-01,  4.9308e-01, -3.4689e-01],\n",
      "        [-7.5336e-02, -4.9705e-01, -1.4284e-01],\n",
      "        [ 8.3428e-02,  1.7386e-01,  3.1407e-01],\n",
      "        [ 1.5752e-01,  2.7621e-01, -5.6334e-01],\n",
      "        [-1.0964e-01,  7.5627e-02,  1.6188e-01],\n",
      "        [-1.5506e-01,  5.6884e-01,  1.7748e-01],\n",
      "        [-5.5482e-01,  3.6075e-01, -3.3695e-01],\n",
      "        [-4.8778e-01, -5.3633e-01,  1.1869e-02],\n",
      "        [ 5.1882e-01, -5.6977e-01,  3.4694e-02],\n",
      "        [ 6.5418e-02,  4.8176e-01,  1.0448e-01],\n",
      "        [ 2.3476e-01, -1.4383e-01, -5.5298e-01],\n",
      "        [-5.1313e-01, -3.8696e-01,  3.5640e-01],\n",
      "        [-5.5217e-01,  5.4147e-01,  5.3686e-01],\n",
      "        [ 4.8653e-01, -1.8099e-01, -1.7321e-01],\n",
      "        [ 2.0404e-01, -4.2145e-01,  3.2903e-01],\n",
      "        [ 4.7111e-01,  5.4503e-01, -4.3718e-01],\n",
      "        [ 5.4522e-01, -2.0409e-01,  1.0922e-01],\n",
      "        [ 1.8141e-01, -9.0434e-02, -5.1677e-01],\n",
      "        [ 7.4015e-02,  3.1379e-01,  5.0994e-01],\n",
      "        [-5.7479e-01,  4.8855e-01, -3.7650e-01],\n",
      "        [-1.3610e-01,  7.1793e-02,  4.2190e-01],\n",
      "        [-4.2045e-01,  5.0904e-01, -1.8124e-01],\n",
      "        [-4.5605e-02, -2.9486e-01, -1.1264e-01],\n",
      "        [-5.1886e-01,  3.2462e-01, -3.7722e-01],\n",
      "        [-2.3545e-01, -5.7446e-01,  2.8257e-02],\n",
      "        [-1.6978e-01, -3.1613e-02, -3.4229e-01],\n",
      "        [ 5.5990e-01,  3.7945e-01,  3.1898e-01],\n",
      "        [-2.5770e-01,  5.7185e-01, -3.9791e-01],\n",
      "        [-5.2516e-01, -1.0219e-01, -7.1644e-02],\n",
      "        [ 2.2589e-01,  7.0757e-04,  2.8369e-01],\n",
      "        [-5.3606e-02, -2.6124e-01,  5.5487e-01],\n",
      "        [ 2.9217e-01, -3.5847e-01,  1.6007e-01],\n",
      "        [-5.6703e-02,  3.1445e-02,  1.0225e-03],\n",
      "        [-8.1185e-03,  3.4026e-01,  3.6825e-01],\n",
      "        [-4.7380e-01,  3.4844e-02, -2.7184e-01],\n",
      "        [-6.9082e-05, -2.0871e-01, -5.4755e-01],\n",
      "        [ 1.4585e-01, -1.4597e-01,  1.5713e-01],\n",
      "        [ 6.3636e-02, -4.5300e-06,  5.7110e-01],\n",
      "        [-2.7443e-01, -3.3724e-01, -3.1055e-01],\n",
      "        [-4.2625e-01, -3.5695e-01,  1.5704e-01],\n",
      "        [-3.3183e-01,  4.2785e-01,  3.6037e-01],\n",
      "        [-5.5534e-01, -1.0650e-01, -3.7363e-01],\n",
      "        [ 2.7451e-01,  3.1751e-01, -1.5392e-01],\n",
      "        [ 2.7880e-01, -3.1141e-01, -4.9716e-01],\n",
      "        [-1.3680e-01, -2.2493e-02,  4.0634e-01],\n",
      "        [ 4.5646e-01, -1.4306e-01, -5.4866e-01],\n",
      "        [-6.8920e-02,  8.8804e-02,  4.4171e-01],\n",
      "        [ 1.7595e-01,  2.8831e-01,  2.2006e-02],\n",
      "        [ 3.1232e-01, -3.5958e-01, -3.8234e-01],\n",
      "        [ 4.9699e-01,  2.6671e-01,  1.5479e-01],\n",
      "        [-5.0606e-01,  5.4756e-01,  9.9059e-02],\n",
      "        [ 5.0256e-01, -1.3579e-01, -4.6333e-01],\n",
      "        [-9.4284e-02, -5.6377e-01,  7.6166e-02],\n",
      "        [-3.0692e-01, -3.6770e-01,  2.8051e-01],\n",
      "        [ 4.3520e-01,  4.2228e-02,  1.2425e-02],\n",
      "        [ 2.1338e-01,  5.6114e-01,  3.8831e-01],\n",
      "        [ 1.1012e-01, -3.0770e-01, -3.3375e-02],\n",
      "        [ 4.8911e-02, -2.2691e-01, -3.0759e-01],\n",
      "        [ 2.8320e-02,  3.0806e-01,  3.3659e-01],\n",
      "        [-1.0354e-01,  1.3707e-02, -1.2190e-01],\n",
      "        [-1.8295e-01,  4.9392e-01, -4.3835e-01],\n",
      "        [ 4.4719e-01, -4.2735e-01, -1.6305e-01],\n",
      "        [ 5.2538e-01,  3.1144e-01, -1.1320e-01],\n",
      "        [ 2.7046e-01, -1.6091e-01,  1.1963e-01],\n",
      "        [-8.9472e-02,  3.8803e-01, -1.7089e-01],\n",
      "        [ 3.5551e-02,  7.7797e-02,  5.4806e-01]])\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# testing forward pass\n",
    "\n",
    "\n",
    "input_tensor = torch.tensor([[-0.9994, -0.0352,  0.2698],\n",
    "        [-0.9947, -0.1029, -0.0644],\n",
    "        [-0.9969,  0.0783, -0.1867],\n",
    "        [-1.0000, -0.0034, -0.2585],\n",
    "        [-0.9993,  0.0362, -0.3160]])\n",
    "\n",
    "#input_tensor = torch.tensor([[-0.9994, -0.0352,  0.2698, -0.0068]])\n",
    "\n",
    "# different state dicts for Q1 and Q2\n",
    "\n",
    "print(agent.actor.NN_actor(input_tensor))\n",
    "\n",
    "print(agent.actor.NN_actor.state_dict()['input.weight'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Running episode:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ss/2547pfrs2tv5kkvqzk0wydg00000gn/T/ipykernel_78071/717656735.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n",
      "/var/folders/ss/2547pfrs2tv5kkvqzk0wydg00000gn/T/ipykernel_78071/717656735.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  std = torch.tensor(std)\n",
      "/var/folders/ss/2547pfrs2tv5kkvqzk0wydg00000gn/T/ipykernel_78071/717656735.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean = torch.tensor(mean)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter scale (Tensor of shape (1,)) of distribution Normal(loc: tensor([0.0753]), scale: tensor([nan])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([nan])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/danaebroustail/Desktop/PAI/Projects/Task 4/PAI-Task4/solution.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m EP \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(TRAIN_EPISODES):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning episode: \u001b[39m\u001b[39m\"\u001b[39m, EP)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     run_episode(env, agent, \u001b[39mNone\u001b[39;49;00m, verbose, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Desktop/PAI/Projects/Task 4/PAI-Task4/utils.py:113\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, agent, rec, verbose, train)\u001b[0m\n\u001b[1;32m    111\u001b[0m episode_return, truncated \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated:\n\u001b[0;32m--> 113\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state, train)\n\u001b[1;32m    115\u001b[0m     state_prime, reward, _, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m train:\n",
      "\u001b[1;32m/Users/danaebroustail/Desktop/PAI/Projects/Task 4/PAI-Task4/solution.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(s)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m#Import action from the actor and discard the log probability here, possibly used elsewhere\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mget_action_and_log_prob(s, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# only get one action -> we have to sample in get_action_and_log_prob\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m#Convert the returned tensor action to an nd.array\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32m/Users/danaebroustail/Desktop/PAI/Projects/Task 4/PAI-Task4/solution.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m log_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclamp_log_std(torch\u001b[39m.\u001b[39mlog(std))   \u001b[39m#The log of the standard deviation must be clamped not the standard deviation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(log_std)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mnormal\u001b[39m.\u001b[39;49mNormal(mean, std)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:  \u001b[39m#We aren't sure about the placement of the clamping, as it makes a difference for the probability, what its std is\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m#action = np.random.normal(mean,std)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m#eps = np.random.normal(0,1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39m#action = std*eps + mean\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39m# Any distribution with .has_rsample == True could work based on the application\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danaebroustail/Desktop/PAI/Projects/Task%204/PAI-Task4/solution.ipynb#X12sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     action \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mrsample() \u001b[39m#rsample includes the reparametrization trick\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PAI/Projects/Task 4/PAI-Task4/.venv/lib/python3.10/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/Desktop/PAI/Projects/Task 4/PAI-Task4/.venv/lib/python3.10/site-packages/torch/distributions/distribution.py:62\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     60\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     61\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 62\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m             )\n\u001b[1;32m     69\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter scale (Tensor of shape (1,)) of distribution Normal(loc: tensor([0.0753]), scale: tensor([nan])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([nan])"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "env = get_env(g=10.0, train=True)\n",
    "\n",
    "for EP in range(TRAIN_EPISODES):\n",
    "    print(\"Running episode: \", EP)\n",
    "    run_episode(env, agent, None, verbose, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.wrappers.time_limit.TimeLimit"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
