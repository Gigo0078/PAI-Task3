{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import warnings\n",
    "from typing import Union\n",
    "from utils import ReplayBuffer, get_env, run_episode\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPISODES = 50\n",
    "TEST_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    This class implements a neural network with a variable number of hidden layers and hidden units.\n",
    "    You may use this function to parametrize your policy and critic networks.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_size: int, \n",
    "                                hidden_layers: int, activation: str):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # TODO: Implement this function which should define a neural network \n",
    "        # with a variable number of hidden layers and hidden units.\n",
    "        # Here you should define layers which your network will use.\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh()\n",
    "            }\n",
    "        self.activation = self.activations[activation]\n",
    "        self.input = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.hidden_size,self.hidden_size) for i in range(self.hidden_layers)])\n",
    "        self.putput = nn.Linear(self.hidden_size, self.output_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement the forward pass for the neural network you have defined.\n",
    "        #pass\n",
    "        s = self.input(s)\n",
    "        s = self.activation(s)\n",
    "        for i in range(0,self.hidden_layers):\n",
    "            s = self.linears[i](s)\n",
    "            s = self.activation(s)\n",
    "        s = self.putput(s)\n",
    "        s = self.activation(s)\n",
    "        #log_s = nn.Softmax(s)\n",
    "        return s #, log_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,hidden_size: int, hidden_layers: int, actor_lr: float,\n",
    "                state_dim: int = 3, action_dim: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.actor_lr = actor_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.setup_actor()\n",
    "\n",
    "    def setup_actor(self):\n",
    "        '''\n",
    "        This function sets up the actor network in the Actor class.\n",
    "        '''\n",
    "        # TODO: Implement this function which sets up the actor network. \n",
    "        # Take a look at the NeuralNetwork class in utils.py. \n",
    "        #pass\n",
    "        self.NN_actor = NeuralNetwork(self.state_dim, 2*self.action_dim, self.hidden_size, self.hidden_layers, \"relu\")\n",
    "        #self.NN_actor = self.agent.trainable_params.optimizer\n",
    "\n",
    "    def clamp_log_std(self, log_std: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param log_std: torch.Tensor, log_std of the policy.\n",
    "        Returns:\n",
    "        :param log_std: torch.Tensor, log_std of the policy clamped between LOG_STD_MIN and LOG_STD_MAX.\n",
    "        '''\n",
    "        return torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "    def get_action_and_log_prob(self, state: torch.Tensor, deterministic: bool) -> (torch.Tensor, torch.Tensor):\n",
    "        '''\n",
    "        :param state: torch.Tensor, state of the agent\n",
    "        :param deterministic: boolean, if true return a deterministic action \n",
    "                                otherwise sample from the policy distribution.\n",
    "        Returns:\n",
    "        :param action: torch.Tensor, action the policy returns for the state.\n",
    "        :param log_prob: log_probability of the the action.\n",
    "        '''\n",
    "        \n",
    "\n",
    "        assert state.shape == (3,) or state.shape[1] == self.state_dim, 'State passed to this method has a wrong shape'\n",
    "        action , log_prob = torch.zeros(state.shape[0]), torch.ones(state.shape[0])\n",
    "        # TODO: Implement this function which returns an action and its log probability.\n",
    "        # If working with stochastic policies, make sure that its log_std are clamped \n",
    "        # using the clamp_log_std function.\n",
    "        with torch.no_grad():\n",
    "\n",
    "            state = torch.tensor(state)\n",
    "            mean, std = self.NN_actor(state)\n",
    "            log_std = self.clamp_log_std(np.log(std))   #The log of the standard deviation must be clamped not the standard deviation\n",
    "            std = np.exp(log_std)\n",
    "\n",
    "            if deterministic == False:  #We aren't sure about the placement of the clamping, as it makes a difference for the probability, what its std is\n",
    "                action = np.random.normal(mean,std)\n",
    "                action = torch.tensor([action])\n",
    "\n",
    "            else:\n",
    "                action = mean\n",
    "\n",
    "            prob = norm(mean,std).pdf(action)   #Have to add scipy.stats.norm to requirements somehow\n",
    "            log_prob = np.log(prob)\n",
    "\n",
    "        action = action.reshape((self.action_dim,))\n",
    "        log_prob = log_prob.reshape((self.action_dim,))\n",
    "\n",
    "        assert action.shape == (self.action_dim, ) and \\\n",
    "            log_prob.shape == (self.action_dim, ), 'Incorrect shape for action or log_prob.'\n",
    "        \n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christoph\\AppData\\Local\\Temp\\ipykernel_9992\\133230202.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n",
      "C:\\Users\\Christoph\\AppData\\Local\\Temp\\ipykernel_9992\\133230202.py:54: RuntimeWarning: divide by zero encountered in log\n",
      "  log_std = self.clamp_log_std(np.log(std))   #The log of the standard deviation must be clamped not the standard deviation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-1.8826e-09]), array([18.66393665]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "\n",
    "agent.actor.get_action_and_log_prob(torch.tensor([1.43, 1.55, 1.99]), deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, hidden_size: int, \n",
    "                 hidden_layers: int, critic_lr: int, state_dim: int = 3, \n",
    "                    action_dim: int = 1,device: torch.device = torch.device('cpu')):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.critic_lr = critic_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.setup_critic()\n",
    "\n",
    "    def setup_critic(self):\n",
    "        # TODO: Implement this function which sets up the critic(s). Take a look at the NeuralNetwork \n",
    "        # class in utils.py. Note that you can have MULTIPLE critic networks in this class.\n",
    "        #pass\n",
    "        #We set the output to 1, but are not sure if the expected value returns a vector\n",
    "        #self.NN_critic_lr = self.critic_lr\n",
    "        self.NN_critic = NeuralNetwork(input_dim = self.state_dim, output_dim=1, hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, activation=\"relu\")\n",
    "        self.optimizer = optim.Adam(self.NN_critic.parameters(),lr = self.critic_lr)\n",
    "        #self.temperature = TrainableParameter(init_param=0.005, lr_param=0.1, train_param=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableParameter:\n",
    "    '''\n",
    "    This class could be used to define a trainable parameter in your method. You could find it \n",
    "    useful if you try to implement the entropy temerature parameter for SAC algorithm.\n",
    "    '''\n",
    "    def __init__(self, init_param: float, lr_param: float, \n",
    "                 train_param: bool, device: torch.device = torch.device('cpu')):\n",
    "        \n",
    "        self.log_param = torch.tensor(np.log(init_param), requires_grad=train_param, device=device)\n",
    "        self.optimizer = optim.Adam([self.log_param], lr=lr_param)\n",
    "\n",
    "    def get_param(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_param)\n",
    "\n",
    "    def get_log_param(self) -> torch.Tensor:\n",
    "        return self.log_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Environment variables. You don't need to change this.\n",
    "        self.state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "        self.action_dim = 1  # [torque] in[-1,1]\n",
    "        self.batch_size = 200\n",
    "        self.min_buffer_size = 1000\n",
    "        self.max_buffer_size = 100000\n",
    "        # If your PC possesses a GPU, you should be able to use it for training, \n",
    "        # as self.device should be 'cuda' in that case.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device: {}\".format(self.device))\n",
    "        self.memory = ReplayBuffer(self.min_buffer_size, self.max_buffer_size, self.device)\n",
    "        \n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        # TODO: Setup off-policy agent with policy and critic classes. \n",
    "        # Feel free to instantiate any other parameters you feel you might need.   \n",
    "        #pass\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_size = 256\n",
    "        self.lr = 3E-4\n",
    "        self.actor = Actor(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        self.critic_Q2 = Critic(state_dim=self.state_dim+self.action_dim,hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, critic_lr=self.lr)\n",
    "        self.critic_Q1 = Critic(state_dim=self.state_dim+self.action_dim,hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, critic_lr=self.lr)\n",
    "        #self.critic = Critic(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        #self.trainable_params = TrainableParameter(init_param: float, self.lr, train_param: bool)\n",
    "        #Name parameters from the paper\n",
    "        #self.log_prob = []\n",
    "        self.Tau = 0.005\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def get_action(self, s: np.ndarray, train: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param s: np.ndarray, state of the pendulum. shape (3, )\n",
    "        :param train: boolean to indicate if you are in eval or train mode. \n",
    "                    You can find it useful if you want to sample from deterministic policy.\n",
    "        :return: np.ndarray,, action to apply on the environment, shape (1,)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a function that returns an action from the policy for the state s.\n",
    "        #action = np.random.uniform(-1, 1, (1,))\n",
    "        #Convert the state to a torch tensor, which is the required input for the actor\n",
    "        #s = torch.tensor(s)\n",
    "        #Import action from the actor and discard the log probability here, possibly used elsewhere\n",
    "        s = torch.tensor(s)\n",
    "        #s = s.detach().numpy()\n",
    "        action, _ = self.actor.get_action_and_log_prob(s, not(train))\n",
    "        # only get one action -> we have to sample in get_action_and_log_prob\n",
    "        #Convert the returned tensor action to an nd.array\n",
    "        action = action.numpy()\n",
    "        #Need log probability for something -------> ?\n",
    "\n",
    "        assert action.shape == (1,), 'Incorrect action shape.'\n",
    "        assert isinstance(action, np.ndarray ), 'Action dtype must be np.ndarray' \n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    def run_gradient_update_step(object: Union[Actor, Critic], loss: torch.Tensor):\n",
    "        '''\n",
    "        This function takes in a object containing trainable parameters and an optimizer, \n",
    "        and using a given loss, runs one step of gradient update. If you set up trainable parameters \n",
    "        and optimizer inside the object, you could find this function useful while training.\n",
    "        :param object: object containing trainable parameters and an optimizer\n",
    "        '''\n",
    "        object.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        object.optimizer.step()\n",
    "\n",
    "    def critic_target_update(self, base_net: NeuralNetwork, target_net: NeuralNetwork, \n",
    "                             tau: float, soft_update: bool):\n",
    "        '''\n",
    "        This method updates the target network parameters using the source network parameters.\n",
    "        If soft_update is True, then perform a soft update, otherwise a hard update (copy).\n",
    "        :param base_net: source network\n",
    "        :param target_net: target network\n",
    "        :param tau: soft update parameter\n",
    "        :param soft_update: boolean to indicate whether to perform a soft update or not\n",
    "        '''\n",
    "        for param_target, param in zip(target_net.parameters(), base_net.parameters()):\n",
    "            if soft_update:\n",
    "                param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "            else:\n",
    "                param_target.data.copy_(param.data)\n",
    "\n",
    "    def train_agent(self):  #------------> ? Christoph is a bit confused, but we need to implement phi, psi and theta gradient updates\n",
    "        '''\n",
    "        This function represents one training iteration for the agent. It samples a batch \n",
    "        from the replay buffer,and then updates the policy and critic networks \n",
    "        using the sampled batch.\n",
    "        '''\n",
    "        # TODO: Implement one step of training for the agent.\n",
    "        # Hint: You can use the run_gradient_update_step for each policy and critic.\n",
    "        # Example: self.run_gradient_update_step(self.policy, policy_loss)\n",
    "        # Batch sampling\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch = batch\n",
    "\n",
    "        #Get the temperature - We still need to figure out which network uses this\n",
    "        #alpha = self.critic_Q.temperature.get_param()\n",
    "        alpha = 0.5\n",
    "        reward = 1/alpha * r_batch\n",
    "        #reward = r_batch + alpha * entropy\n",
    "\n",
    "        #Store the basic Psi network - which I guess we still need\n",
    "        #base_net = self.critic_V.NN_critic()\n",
    "\n",
    "        #Bliblablup for loss functions\n",
    "        #Determine Thetas from their according neural networks with the given state input - Should rename the networks accordingly\n",
    "        with torch.no_grad():\n",
    "            #Sample action and its log_prob\n",
    "            next_sampled_action, next_sampled_log_prob = self.actor.get_action_and_log_prob(state=s_prime_batch, deterministic=False)\n",
    "            qf1_next = self.critic.NN_critic_Q1(s_prime_batch,next_sampled_action)   #Takes in the batch state but sampled action\n",
    "            qf2_next = self.critic.NN_critic_Q2(s_prime_batch,next_sampled_action)\n",
    "            # transform into Value function\n",
    "            min_qf_next = torch.min(qf1_next,qf2_next) - next_sampled_log_prob # * alpha #Here have to be careful with the alpha, either we use it to scale the rewards or we include it in the losses\n",
    "            #Q hatfunction\n",
    "            next_q_value = reward + self.gamma * min_qf_next\n",
    "\n",
    "        #Get the current values and optimize with respect to the next ones\n",
    "        qf1 = self.critic.NN_critic_Q1(s_batch,a_batch)  #Might have to use torch.concat here as the input to the critic networks\n",
    "        qf2 = self.critic.NN_critic_Q2(s_batch,a_batch)\n",
    "        #Losses for the competing critic networks, represented by theta 1 and 2\n",
    "        q1_loss = nn.functional.mse_loss(qf1, next_q_value)  #Have to figure out what q1 and q2 are\n",
    "        q2_loss = nn.functional.mse_loss(qf2,next_q_value)\n",
    "\n",
    "        #Sample current action and its log_prob\n",
    "        sampled_action, sampled_log_prob = self.actor.get_action_and_log_prob(state=s_batch, deterministic=False)\n",
    "\n",
    "        Q1_pi = self.critic.NN_critic_Q1(s_batch,sampled_action)\n",
    "        Q2_pi = self.critic.NN_critic_Q2(s_batch,sampled_action)\n",
    "        min_q_pi = torch.min(Q1_pi, Q2_pi)\n",
    "\n",
    "        #Optimize the critic networks\n",
    "        #Run a gradient update step for critic V\n",
    "        # TODO: Implement Critic(s) update here.\n",
    "        self.run_gradient_update_step(self.critic_Q1,q1_loss)\n",
    "        self.run_gradient_update_step(self.critic_Q2,q2_loss)\n",
    "\n",
    "        #Policy loss\n",
    "        # TODO: Implement Policy update here\n",
    "        policy_loss = ((self.alpha * sampled_log_prob) - min_q_pi).mean()\n",
    "        #Gradient update for policy\n",
    "        self.run_gradient_update_step(self.actor,policy_loss)\n",
    "        \n",
    "        #Critic target update step\n",
    "        #self.critic_target_update(base_net,self.critic_V.NN_critic,self.Tau,True)\n",
    "\n",
    "        #I think that at the end, we still have to calculate the rewards and stuff relating to the formula given in the task description no?\n",
    "        \n",
    "        #Since we're performing an update step, we must include this new sampled batch in the neural network\n",
    "        #reward_sampled = -(s_batch^2 + 0.1*s_prime_batch^2 + 0.001*a_batch^2)\n",
    "        #self.actor.NN_actor.train() #Train the actor\n",
    "        #self.critic.NN_critic_V.train() #Train the first critic\n",
    "        #self.critic.NN_critic_Q.train() #Train the second critic\n",
    "\n",
    "        \n",
    "        #policy = np.transpose(np.array([np.cos(s_batch),np.sin(s_batch),s_prime_batch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "#Env determines the Markov Decision Process\n",
    "env = get_env(g=10.0, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christoph\\AppData\\Local\\Temp\\ipykernel_9992\\732128340.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Fails as of right now, as we need to obtain log_prob\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Christoph\\OneDrive - ETH Zurich\\Documents\\ETH\\Probabilistic AI\\PAI-Task4\\utils.py:113\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, agent, rec, verbose, train)\u001b[0m\n\u001b[0;32m    111\u001b[0m episode_return, truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n\u001b[1;32m--> 113\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     state_prime, reward, _, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train:\n",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[1;34m(self, s, train)\u001b[0m\n\u001b[0;32m     44\u001b[0m s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(s)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#s = s.detach().numpy()\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action_and_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# only get one action -> we have to sample in get_action_and_log_prob\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#Convert the returned tensor action to an nd.array\u001b[39;00m\n\u001b[0;32m     49\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m, in \u001b[0;36mActor.get_action_and_log_prob\u001b[1;34m(self, state, deterministic)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#How are probability and the log standard deviation related?\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:  \u001b[38;5;66;03m#We aren't sure about the placement of the clamping, as it makes a difference for the probability, what its std is\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     log_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclamp_log_std(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m)   \u001b[38;5;66;03m#The log of the standard deviation must be clamped not the standard deviation\u001b[39;00m\n\u001b[0;32m     54\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(log_std)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#Sample an action from the Gaussian\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Christoph\\anaconda3\\envs\\PAItask4\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "save_video = False\n",
    "verbose = True\n",
    "#Fails as of right now, as we need to obtain log_prob\n",
    "run_episode(env, agent, None, verbose, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.wrappers.time_limit.TimeLimit"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
