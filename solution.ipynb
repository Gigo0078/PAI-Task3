{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import warnings\n",
    "from typing import Union\n",
    "from utils import ReplayBuffer, get_env, run_episode\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPISODES = 50\n",
    "TEST_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    This class implements a neural network with a variable number of hidden layers and hidden units.\n",
    "    You may use this function to parametrize your policy and critic networks.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_size: int, \n",
    "                                hidden_layers: int, activation: str):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # TODO: Implement this function which should define a neural network \n",
    "        # with a variable number of hidden layers and hidden units.\n",
    "        # Here you should define layers which your network will use.\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh()\n",
    "            }\n",
    "        #self.activation = self.activations[activation]\n",
    "        self.activation = nn.ReLU()\n",
    "        self.input = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.hidden_size,self.hidden_size) for i in range(self.hidden_layers)])\n",
    "        self.putput = nn.Linear(self.hidden_size, self.output_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement the forward pass for the neural network you have defined.\n",
    "        #pass\n",
    "        s = self.input(s)\n",
    "        s = self.activation(s)\n",
    "        #print(\"after activation\", s)\n",
    "        for i in range(0,self.hidden_layers):\n",
    "            \n",
    "            s = self.linears[i](s)\n",
    "            #print(\"linear layer\", s)\n",
    "\n",
    "            s = self.activation(s)\n",
    "            #print(\"activation in linear layer\", s)\n",
    "            \n",
    "        s = self.putput(s)\n",
    "        #print(\"output is\", s)\n",
    "        s = self.activation(s)\n",
    "        #print(\"after activation output layer\", s)\n",
    "        #log_s = nn.Softmax(s)\n",
    "        return s #, log_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,hidden_size: int, hidden_layers: int, actor_lr: float,\n",
    "                state_dim: int = 3, action_dim: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.actor_lr = actor_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.setup_actor()\n",
    "\n",
    "    def setup_actor(self):\n",
    "        '''\n",
    "        This function sets up the actor network in the Actor class.\n",
    "        '''\n",
    "        # TODO: Implement this function which sets up the actor network. \n",
    "        # Take a look at the NeuralNetwork class in utils.py. \n",
    "        #pass\n",
    "        self.NN_actor = NeuralNetwork(input_dim=self.state_dim, output_dim=2*self.action_dim, hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, activation=\"relu\")\n",
    "        self.NN_actor.to(self.device)\n",
    "        self.optimizer= optim.Adam(self.NN_actor.parameters(),lr = self.actor_lr)\n",
    "        self.temperature = TrainableParameter(init_param=0.005, lr_param=0.1, train_param=True)\n",
    "\n",
    "    def clamp_log_std(self, log_std: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param log_std: torch.Tensor, log_std of the policy.\n",
    "        Returns:\n",
    "        :param log_std: torch.Tensor, log_std of the policy clamped between LOG_STD_MIN and LOG_STD_MAX.\n",
    "        '''\n",
    "        return torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "    def get_action_and_log_prob(self, state: torch.Tensor, \n",
    "                                deterministic: bool = False) -> (torch.Tensor, torch.Tensor):\n",
    "        '''\n",
    "        :param state: torch.Tensor, state of the agent\n",
    "        :param deterministic: boolean, if true return a deterministic action \n",
    "                                otherwise sample from the policy distribution.\n",
    "        Returns:\n",
    "        :param action: torch.Tensor, action the policy returns for the state.\n",
    "        :param log_prob: log_probability of the the action.\n",
    "        '''\n",
    "        #print(\"input state is\", state)\n",
    "        assert state.shape == (3,) or state.shape[1] == self.state_dim, 'State passed to this method has a wrong shape'\n",
    "        action , log_prob = torch.zeros(state.shape[0]), torch.ones(state.shape[0])\n",
    "        # TODO: Implement this function which returns an action and its log probability.\n",
    "        # If working with stochastic policies, make sure that its log_std are clamped \n",
    "        # using the clamp_log_std function.\n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            state = torch.tensor(state)\n",
    "\n",
    "            #print(\"NN_actor\", self.NN_actor )\n",
    "            #print(\"output looks like\", self.NN_actor(state))\n",
    "            #print(\"shape of output\", self.NN_actor(state).shape)\n",
    "            #output = self.NN_actor(state)\n",
    "            mean, std = torch.chunk(self.NN_actor(state), 2, dim=-1)#.to(self.device)\n",
    "\n",
    "            std = torch.tensor(std)\n",
    "            mean = torch.tensor(mean)\n",
    "            \n",
    "            log_std = self.clamp_log_std(torch.log(std))   #The log of the standard deviation must be clamped not the standard deviation\n",
    "            std = torch.exp(log_std)\n",
    "            dist = torch.distributions.normal.Normal(mean, std)\n",
    "\n",
    "            if deterministic == False:  #We aren't sure about the placement of the clamping, as it makes a difference for the probability, what its std is\n",
    "                #action = np.random.normal(mean,std)\n",
    "                #eps = np.random.normal(0,1)\n",
    "                #action = std*eps + mean\n",
    "                # Any distribution with .has_rsample == True could work based on the application\n",
    "                action = dist.rsample() #rsample includes the reparametrization trick\n",
    "                action = torch.tanh(action)\n",
    "                #action = torch.normal(mean, std)\n",
    "                #action = torch.tanh(action)\n",
    "\n",
    "            else:\n",
    "                action = mean\n",
    "\n",
    "                        \n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # print(\"m:\", mean)\n",
    "            # print(\"action is:\", action)\n",
    "            # print(\"Get your std here:\", std)\n",
    "            # print(\"that's the log-probability\", log_prob)\n",
    "\n",
    "        action = action.reshape((self.action_dim,))\n",
    "        log_prob = torch.tensor(log_prob.reshape((self.action_dim,)))\n",
    "\n",
    "        assert action.shape == (self.action_dim, ) and \\\n",
    "            log_prob.shape == (self.action_dim, ), 'Incorrect shape for action or log_prob.'\n",
    "        \n",
    "        return action, log_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, hidden_size: int, \n",
    "                 hidden_layers: int, critic_lr: int, state_dim: int = 3, \n",
    "                    action_dim: int = 1,device: torch.device = torch.device('cpu')):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.critic_lr = critic_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.setup_critic()\n",
    "\n",
    "    def setup_critic(self):\n",
    "        # TODO: Implement this function which sets up the critic(s). Take a look at the NeuralNetwork \n",
    "        # class in utils.py. Note that you can have MULTIPLE critic networks in this class.\n",
    "\n",
    "        self.NN_critic = NeuralNetwork(input_dim = self.state_dim, output_dim=1, hidden_size=self.hidden_size, hidden_layers=self.hidden_layers, activation=\"relu\")\n",
    "        self.NN_critic.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.NN_critic.parameters(),lr = self.critic_lr)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableParameter:\n",
    "    '''\n",
    "    This class could be used to define a trainable parameter in your method. You could find it \n",
    "    useful if you try to implement the entropy temerature parameter for SAC algorithm.\n",
    "    '''\n",
    "    def __init__(self, init_param: float, lr_param: float, \n",
    "                 train_param: bool, device: torch.device = torch.device('cpu')):\n",
    "        \n",
    "        self.log_param = torch.tensor(np.log(init_param), requires_grad=train_param, device=device)\n",
    "        self.optimizer = optim.Adam([self.log_param], lr=lr_param)\n",
    "\n",
    "    def get_param(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_param)\n",
    "\n",
    "    def get_log_param(self) -> torch.Tensor:\n",
    "        return self.log_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Environment variables. You don't need to change this.\n",
    "        self.state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "        self.action_dim = 1  # [torque] in[-1,1]\n",
    "        self.batch_size = 200\n",
    "        self.min_buffer_size = 1000\n",
    "        self.max_buffer_size = 100000\n",
    "        # If your PC possesses a GPU, you should be able to use it for training, \n",
    "        # as self.device should be 'cuda' in that case.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device: {}\".format(self.device))\n",
    "        self.memory = ReplayBuffer(self.min_buffer_size, self.max_buffer_size, self.device)\n",
    "        \n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        # TODO: Setup off-policy agent with policy and critic classes. \n",
    "        # Feel free to instantiate any other parameters you feel you might need.   \n",
    "        #pass\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_size = 256\n",
    "        self.lr = 3E-3\n",
    "\n",
    "        self.actor = Actor(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        self.critic_Q2 = Critic(state_dim=self.state_dim+self.action_dim,\n",
    "                                hidden_size=self.hidden_size, \n",
    "                                hidden_layers=self.hidden_layers,\n",
    "                                critic_lr=self.lr)\n",
    "        \n",
    "        self.critic_Q1 = Critic(state_dim=self.state_dim+self.action_dim,\n",
    "                                hidden_size=self.hidden_size,\n",
    "                                hidden_layers=self.hidden_layers,\n",
    "                                critic_lr=self.lr)\n",
    "        #self.critic = Critic(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        #self.trainable_params = TrainableParameter(init_param: float, self.lr, train_param: bool)\n",
    "        #Name parameters from the paper\n",
    "        #self.log_prob = []\n",
    "        self.Tau = 0.005\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def get_action(self, s: np.ndarray, train: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param s: np.ndarray, state of the pendulum. shape (3, )\n",
    "        :param train: boolean to indicate if you are in eval or train mode. \n",
    "                    You can find it useful if you want to sample from deterministic policy.\n",
    "        :return: np.ndarray,, action to apply on the environment, shape (1,)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a function that returns an action from the policy for the state s.\n",
    "        #action = np.random.uniform(-1, 1, (1,))\n",
    "        #Convert the state to a torch tensor, which is the required input for the actor\n",
    "        s = torch.tensor(s)\n",
    "        #Import action from the actor and discard the log probability here, possibly used elsewhere\n",
    "        action, _ = self.actor.get_action_and_log_prob(s, False)\n",
    "        # only get one action -> we have to sample in get_action_and_log_prob\n",
    "        #Convert the returned tensor action to an nd.array\n",
    "        action = action.clone().detach().numpy()\n",
    "        #Need log probability for something -------> ?\n",
    "\n",
    "        assert action.shape == (1,), 'Incorrect action shape.'\n",
    "        assert isinstance(action, np.ndarray ), 'Action dtype must be np.ndarray' \n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    # loss: 200 x 1\n",
    "    def run_gradient_update_step(object: Union[Actor, Critic], loss: torch.Tensor):\n",
    "        '''\n",
    "        This function takes in a object containing trainable parameters and an optimizer, \n",
    "        and using a given loss, runs one step of gradient update. If you set up trainable parameters \n",
    "        and optimizer inside the object, you could find this function useful while training.\n",
    "        :param object: object containing trainable parameters and an optimizer\n",
    "        '''\n",
    "        object.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        object.optimizer.step()\n",
    "\n",
    "    def critic_target_update(self, base_net: NeuralNetwork, target_net: NeuralNetwork, \n",
    "                             tau: float, soft_update: bool):\n",
    "        '''\n",
    "        This method updates the target network parameters using the source network parameters.\n",
    "        If soft_update is True, then perform a soft update, otherwise a hard update (copy).\n",
    "        :param base_net: source network\n",
    "        :param target_net: target network\n",
    "        :param tau: soft update parameter\n",
    "        :param soft_update: boolean to indicate whether to perform a soft update or not\n",
    "        '''\n",
    "        for param_target, param in zip(target_net.parameters(), base_net.parameters()):\n",
    "            if soft_update:\n",
    "                param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "            else:\n",
    "                param_target.data.copy_(param.data)\n",
    "\n",
    "    def train_agent(self): \n",
    "        '''\n",
    "        This function represents one training iteration for the agent. It samples a batch \n",
    "        from the replay buffer,and then updates the policy and critic networks \n",
    "        using the sampled batch.\n",
    "        '''\n",
    "        # TODO: Implement one step of training for the agent.\n",
    "        # Hint: You can use the run_gradient_update_step for each policy and critic.\n",
    "        # Example: self.run_gradient_update_step(self.policy, policy_loss)\n",
    "        # Batch sampling\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch = batch\n",
    "\n",
    "        print(\"#############################\")\n",
    "        print(\"train_agent\")\n",
    "        print(\"#############################\")\n",
    "\n",
    "        #Get the temperature - We still need to figure out which network uses this\n",
    "        alpha = self.actor.temperature.get_param()\n",
    "        print(\"alpha before optimization\", alpha)\n",
    "        #alpha = torch.tensor(0.5)\n",
    "        reward =  1/alpha * r_batch # smth to investigate\n",
    "        print(\"modified reward\", reward[0:5, :])\n",
    "        #reward = r_batch + alpha * entropy <--\n",
    "\n",
    "        #Store the basic Psi network - which I guess we still need\n",
    "        base_net1 = NeuralNetwork(input_dim = self.state_dim + self.action_dim, \n",
    "                                  output_dim = 1, \n",
    "                                  hidden_size = 256,\n",
    "                                  hidden_layers = 2,\n",
    "                                  activation=\"relu\").to(self.device) #self.critic_Q1.NN_critic #self.critic_Q1.NN_critic\n",
    "        \n",
    "        base_net2 = NeuralNetwork(input_dim = self.state_dim + self.action_dim, \n",
    "                                  output_dim = 1, \n",
    "                                  hidden_size = 256,\n",
    "                                  hidden_layers = 2,\n",
    "                                  activation=\"relu\").to(self.device) #self.critic_Q2.NN_critic\n",
    "\n",
    "        base_net1.load_state_dict(copy.deepcopy(self.critic_Q1.NN_critic.state_dict()))\n",
    "        base_net1.to(self.device)\n",
    "        base_net2.load_state_dict(copy.deepcopy(self.critic_Q2.NN_critic.state_dict()))\n",
    "        base_net2.to(self.device)\n",
    "\n",
    "        print(\"Q1 before gradient\", base_net1.state_dict()['putput.weight'][0,:5])\n",
    "\n",
    "        #Optimize the critic networks\n",
    "        #Run a gradient update step for critic V\n",
    "        # TODO: Implement Critic(s) update here.\n",
    "\n",
    "       \n",
    "        with torch.no_grad():\n",
    "\n",
    "            results_list = [self.actor.get_action_and_log_prob(state, False) for state in s_prime_batch] \n",
    "            \n",
    "            next_sampled_action, next_sampled_log_prob = zip(*results_list)\n",
    "\n",
    "            next_sampled_action = torch.tensor(next_sampled_action).flatten().reshape(self.batch_size, 1)\n",
    "            next_sampled_log_prob = torch.tensor(next_sampled_log_prob).flatten().reshape(self.batch_size, 1)\n",
    "\n",
    "            print(\"next_sampled_action\",next_sampled_action[0:5,:])\n",
    "            print(\"next_sampled_log_prob\", next_sampled_log_prob[0:5,:])\n",
    "\n",
    "            input = torch.cat((s_prime_batch, next_sampled_action), dim = 1).to(self.device)\n",
    "            print(\"input looks like\", input[0:5,])\n",
    "\n",
    "            qf1_next = self.critic_Q1.NN_critic(input)   \n",
    "            qf2_next = self.critic_Q2.NN_critic(input)\n",
    "\n",
    "\n",
    "\n",
    "            print(\"Total number of zero outputs\", (qf1_next == 0).sum(), \"out of\", qf1_next.shape)\n",
    "\n",
    "            min_qf_next = torch.min(qf1_next,qf2_next) - next_sampled_log_prob\n",
    "\n",
    "            print(\"min_qf_next\",min_qf_next[0:5,:])\n",
    "\n",
    "            next_q_value = reward + self.gamma * min_qf_next # 200 x 1\n",
    "\n",
    "        print(\"next_q_value\", next_q_value[0:5,:])\n",
    "\n",
    "        #Get the current values and optimize with respect to the next ones\n",
    "        input_Q = torch.cat((s_batch, a_batch), dim = 1).to(self.device)\n",
    "    \n",
    "        qf1 = self.critic_Q1.NN_critic(input_Q) # 200 x 1\n",
    "        qf2 = self.critic_Q2.NN_critic(input_Q) # 200 x 1\n",
    "\n",
    "        print(\"Total number of zero outputs\", (qf1 == 0).sum(), \"out of\", qf1.shape)\n",
    "\n",
    "        q1_loss = nn.functional.mse_loss(qf1, next_q_value)  \n",
    "        q2_loss = nn.functional.mse_loss(qf2,next_q_value)\n",
    "\n",
    "        print(\"q1 loss\", q1_loss)\n",
    "\n",
    "        self.run_gradient_update_step(self.critic_Q1, q1_loss)\n",
    "        self.run_gradient_update_step(self.critic_Q2, q2_loss)\n",
    "\n",
    "        # print some gradients\n",
    "\n",
    "        print(\"grad of NN_critic Q1 putput weight\", self.critic_Q1.NN_critic.putput.weight.grad)\n",
    "        print(\"grad of NN_critic Q2 putput weight\", self.critic_Q2.NN_critic.putput.weight.grad)\n",
    "        #print(\"grad of NN_critic Q2\", )\n",
    "\n",
    "        #Sample current action and its log_prob\n",
    "        with torch.no_grad():\n",
    "            results_list2 = [self.actor.get_action_and_log_prob(state, False) for state in s_batch]\n",
    "\n",
    "            sampled_action, sampled_log_prob = zip(*results_list2) #self.actor.get_action_and_log_prob(state=s_batch, deterministic=False)\n",
    "        \n",
    "            sampled_action = torch.tensor(sampled_action).flatten().reshape(self.batch_size, 1)\n",
    "            sampled_log_prob = torch.tensor(sampled_log_prob).flatten().reshape(self.batch_size, 1)\n",
    "\n",
    "        input_policy = torch.cat((s_batch, sampled_action), dim = 1).to(self.device)\n",
    "        Q1_pi = self.critic_Q1.NN_critic(input_policy) #s_batch,sampled_action)\n",
    "        Q2_pi = self.critic_Q2.NN_critic(input_policy) #s_batch,sampled_action)\n",
    "        min_q_pi = torch.min(Q1_pi, Q2_pi)\n",
    "        \n",
    "        #Policy loss\n",
    "\n",
    "\n",
    "        # TODO: Implement Policy update here\n",
    "        policy_loss = ((sampled_log_prob) - min_q_pi) # self.alpha * removed\n",
    "        print(\"policy loss\", policy_loss[0:5,])\n",
    "\n",
    "        #Gradient update for policy\n",
    "        self.run_gradient_update_step(self.actor, policy_loss)\n",
    "\n",
    "        print(\"grad of policy network\", self.actor.NN_actor.input.weight.grad)\n",
    "\n",
    "\n",
    "        # print some gradients\n",
    "\n",
    "\n",
    "        # Temperature (alpha) loss\n",
    "        print(\"------ Training temperature -------\")\n",
    "\n",
    "        H = -1.\n",
    "        alpha_loss = - alpha * sampled_log_prob - alpha * H\n",
    "\n",
    "        self.actor.temperature.optimizer.zero_grad()\n",
    "        alpha_loss.mean().backward()\n",
    "        self.actor.temperature.optimizer.step()\n",
    "        \n",
    "        print(\"targetnet --> Q1 after gradient, before soft update\", self.critic_Q1.NN_critic.state_dict()['putput.weight'][0,:5])\n",
    "        #Critic target update step\n",
    "        print(\"basenet1 -->\", base_net1.state_dict()['putput.weight'][0,:5])\n",
    "\n",
    "        self.critic_target_update(base_net1, self.critic_Q1.NN_critic, self.Tau,True)\n",
    "        self.critic_target_update(base_net2, self.critic_Q2.NN_critic, self.Tau,True)\n",
    "\n",
    "        print(\"Q1 after update\", self.critic_Q1.NN_critic.state_dict()['putput.weight'][0,:5])\n",
    "\n",
    "        alpha = self.actor.temperature.get_param()\n",
    "        print(\"alpha after optimization\", alpha)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "OrderedDict([('input.weight', tensor([[ 0.4292, -0.4745,  0.3111, -0.4885],\n",
      "        [-0.4321,  0.3436, -0.4581,  0.4618],\n",
      "        [ 0.3614, -0.2132, -0.1909,  0.0095],\n",
      "        ...,\n",
      "        [-0.2542,  0.4118, -0.4012,  0.3099],\n",
      "        [ 0.4879, -0.4357, -0.4233,  0.4952],\n",
      "        [-0.0798,  0.2009, -0.3189,  0.0798]])), ('input.bias', tensor([-0.1985, -0.0165,  0.0707,  0.2365, -0.0623,  0.2450,  0.3401,  0.3145,\n",
      "         0.2811,  0.0026, -0.2160, -0.3695,  0.2957, -0.3792, -0.0550, -0.3044,\n",
      "         0.3023, -0.0826, -0.2415,  0.0568,  0.1450, -0.0914,  0.2461, -0.0534,\n",
      "        -0.1623,  0.3340,  0.0419,  0.3482, -0.1019, -0.1915,  0.3888,  0.1113,\n",
      "         0.3528, -0.4044, -0.1524, -0.0737, -0.1537,  0.4163, -0.2467, -0.0260,\n",
      "        -0.1308,  0.2903, -0.2966, -0.3550,  0.4081, -0.4936,  0.2655, -0.3223,\n",
      "        -0.3320, -0.4083, -0.4491, -0.1212,  0.1289,  0.0301, -0.1658, -0.2639,\n",
      "         0.3130,  0.1804,  0.4936, -0.0863, -0.0106,  0.1509,  0.4422,  0.3842,\n",
      "         0.4733,  0.1593, -0.4361,  0.2831,  0.3297,  0.2735, -0.3620,  0.2763,\n",
      "        -0.1743, -0.3195,  0.2506, -0.0415,  0.2238,  0.2281,  0.1288,  0.4611,\n",
      "        -0.3085, -0.4076,  0.3866,  0.3864,  0.2010,  0.1481, -0.4836,  0.1799,\n",
      "         0.1296,  0.2324,  0.1514, -0.2812, -0.1995,  0.3848, -0.2790, -0.0905,\n",
      "         0.2684, -0.3259,  0.1436, -0.2068,  0.4069, -0.1001,  0.4045,  0.1248,\n",
      "         0.1707, -0.2674, -0.0297, -0.3180, -0.2430,  0.2310,  0.1928,  0.3323,\n",
      "        -0.4884,  0.1860, -0.2360,  0.3409, -0.0791, -0.1711,  0.3290,  0.0902,\n",
      "         0.4802, -0.1628,  0.2511, -0.4602,  0.1673,  0.0585,  0.2001, -0.0886,\n",
      "         0.2598, -0.2407,  0.1038,  0.0905, -0.4045, -0.2359, -0.0975,  0.0636,\n",
      "        -0.2708,  0.4247, -0.0024, -0.4413,  0.3622,  0.1679, -0.3430, -0.0889,\n",
      "         0.0816,  0.1550,  0.4804,  0.3234, -0.1903, -0.4324, -0.0821,  0.3728,\n",
      "         0.2639, -0.3552, -0.0406, -0.1151,  0.2608,  0.2012, -0.1743,  0.1776,\n",
      "         0.0356, -0.1722,  0.1465, -0.0424,  0.2596, -0.4337, -0.2215,  0.1700,\n",
      "         0.0671, -0.3409, -0.3968,  0.3227,  0.0610,  0.3567,  0.1425, -0.3549,\n",
      "         0.3549, -0.2105, -0.4281,  0.1991, -0.0163,  0.2409, -0.1052, -0.0170,\n",
      "         0.2519,  0.2967, -0.0568,  0.2039,  0.2171, -0.1363,  0.1703,  0.3519,\n",
      "        -0.0593,  0.4543,  0.4243, -0.3189,  0.3872,  0.4023, -0.1136, -0.2945,\n",
      "         0.4350, -0.4201, -0.4940,  0.1354,  0.1367,  0.4202,  0.1703, -0.0140,\n",
      "         0.2326,  0.2595, -0.3491,  0.1426, -0.2071, -0.1311, -0.0054, -0.3321,\n",
      "        -0.0200,  0.2867, -0.0073, -0.2169, -0.1127, -0.1747,  0.1545, -0.4541,\n",
      "        -0.4019,  0.3282, -0.2660,  0.0132, -0.3561, -0.3945, -0.2738,  0.4018,\n",
      "         0.1174, -0.2553,  0.2867, -0.3071,  0.0614,  0.0932,  0.2688, -0.3839,\n",
      "        -0.0105,  0.0828,  0.2679,  0.4779,  0.0520, -0.0609,  0.3590,  0.4822,\n",
      "        -0.1711,  0.3541,  0.4427,  0.2244, -0.1004,  0.1125, -0.4009, -0.0719])), ('linears.0.weight', tensor([[-0.0191,  0.0584, -0.0546,  ..., -0.0441,  0.0361, -0.0426],\n",
      "        [ 0.0250,  0.0116,  0.0036,  ...,  0.0513, -0.0240,  0.0518],\n",
      "        [-0.0104,  0.0473,  0.0171,  ...,  0.0096, -0.0590,  0.0008],\n",
      "        ...,\n",
      "        [ 0.0540,  0.0539,  0.0170,  ..., -0.0205, -0.0571,  0.0239],\n",
      "        [ 0.0442,  0.0568,  0.0268,  ..., -0.0419, -0.0480,  0.0094],\n",
      "        [ 0.0208,  0.0618,  0.0552,  ...,  0.0455, -0.0217, -0.0098]])), ('linears.0.bias', tensor([ 0.0489, -0.0005, -0.0304, -0.0228, -0.0401, -0.0091, -0.0307,  0.0049,\n",
      "         0.0609,  0.0175, -0.0151, -0.0544, -0.0159,  0.0459,  0.0439, -0.0067,\n",
      "         0.0033,  0.0151,  0.0455,  0.0539,  0.0040,  0.0413,  0.0219,  0.0245,\n",
      "        -0.0602,  0.0318, -0.0492,  0.0481, -0.0163, -0.0519, -0.0308, -0.0295,\n",
      "        -0.0152, -0.0041,  0.0545, -0.0338,  0.0046,  0.0077, -0.0265, -0.0068,\n",
      "        -0.0151,  0.0293,  0.0148, -0.0339,  0.0140,  0.0214,  0.0161, -0.0402,\n",
      "         0.0270,  0.0451,  0.0406, -0.0524,  0.0290,  0.0002, -0.0251, -0.0188,\n",
      "         0.0007,  0.0528,  0.0610, -0.0324, -0.0280, -0.0460, -0.0074,  0.0307,\n",
      "         0.0477,  0.0175, -0.0329,  0.0340, -0.0470,  0.0609,  0.0084,  0.0125,\n",
      "         0.0395, -0.0399,  0.0118, -0.0360, -0.0139, -0.0317,  0.0291,  0.0081,\n",
      "         0.0203,  0.0445, -0.0442, -0.0357,  0.0074, -0.0527, -0.0299,  0.0563,\n",
      "        -0.0323,  0.0074, -0.0438, -0.0448, -0.0055, -0.0208,  0.0419, -0.0153,\n",
      "        -0.0086,  0.0006,  0.0625, -0.0475,  0.0380, -0.0471, -0.0218, -0.0039,\n",
      "         0.0587, -0.0499, -0.0135, -0.0071,  0.0336, -0.0615, -0.0199,  0.0327,\n",
      "        -0.0140,  0.0483,  0.0427,  0.0140, -0.0239,  0.0138, -0.0580, -0.0431,\n",
      "         0.0324,  0.0187, -0.0108,  0.0344, -0.0537,  0.0222, -0.0299, -0.0625,\n",
      "        -0.0206, -0.0074, -0.0502,  0.0381,  0.0093, -0.0274,  0.0410,  0.0322,\n",
      "        -0.0217, -0.0312,  0.0122, -0.0326,  0.0225,  0.0373,  0.0264,  0.0037,\n",
      "         0.0396,  0.0224, -0.0360, -0.0513,  0.0157, -0.0125, -0.0508,  0.0301,\n",
      "         0.0039,  0.0395,  0.0236, -0.0309,  0.0054, -0.0180,  0.0167, -0.0565,\n",
      "        -0.0496,  0.0373,  0.0276,  0.0384,  0.0370,  0.0397, -0.0181,  0.0220,\n",
      "         0.0557,  0.0159, -0.0492, -0.0435,  0.0509,  0.0537,  0.0100,  0.0530,\n",
      "         0.0246,  0.0592,  0.0077,  0.0034,  0.0263,  0.0145, -0.0088, -0.0161,\n",
      "        -0.0414,  0.0405,  0.0375, -0.0178,  0.0245, -0.0606,  0.0343, -0.0511,\n",
      "         0.0024, -0.0016, -0.0392,  0.0044, -0.0036, -0.0459, -0.0522, -0.0013,\n",
      "         0.0574, -0.0389, -0.0348, -0.0395,  0.0311,  0.0490,  0.0598,  0.0448,\n",
      "         0.0260, -0.0347, -0.0255, -0.0286, -0.0416,  0.0159,  0.0393,  0.0060,\n",
      "         0.0162, -0.0549,  0.0577,  0.0097,  0.0166, -0.0489,  0.0431, -0.0166,\n",
      "        -0.0449, -0.0276,  0.0300, -0.0201,  0.0140, -0.0268, -0.0317,  0.0396,\n",
      "         0.0351, -0.0373, -0.0396,  0.0328, -0.0094, -0.0018, -0.0309,  0.0399,\n",
      "        -0.0277,  0.0512, -0.0012, -0.0559, -0.0330, -0.0486,  0.0370, -0.0117,\n",
      "         0.0185,  0.0509, -0.0086, -0.0543, -0.0437, -0.0246,  0.0425,  0.0602])), ('linears.1.weight', tensor([[ 0.0248,  0.0425,  0.0529,  ...,  0.0007,  0.0431,  0.0003],\n",
      "        [-0.0201, -0.0227,  0.0351,  ...,  0.0026,  0.0530,  0.0191],\n",
      "        [ 0.0269, -0.0407, -0.0347,  ..., -0.0489,  0.0347,  0.0564],\n",
      "        ...,\n",
      "        [ 0.0431, -0.0438, -0.0231,  ..., -0.0289,  0.0075,  0.0283],\n",
      "        [ 0.0452, -0.0138, -0.0305,  ..., -0.0089,  0.0538,  0.0593],\n",
      "        [-0.0594,  0.0072,  0.0336,  ..., -0.0338, -0.0452, -0.0323]])), ('linears.1.bias', tensor([ 0.0354, -0.0262,  0.0473,  0.0611,  0.0288, -0.0506,  0.0565,  0.0262,\n",
      "         0.0242, -0.0190,  0.0329,  0.0598, -0.0274,  0.0177, -0.0196, -0.0409,\n",
      "        -0.0260, -0.0117,  0.0088,  0.0386, -0.0363,  0.0418, -0.0547,  0.0157,\n",
      "        -0.0121,  0.0029,  0.0042,  0.0175,  0.0585,  0.0185, -0.0298, -0.0542,\n",
      "         0.0597, -0.0337, -0.0214,  0.0513,  0.0532, -0.0370,  0.0607,  0.0526,\n",
      "         0.0095,  0.0159, -0.0399,  0.0018,  0.0162, -0.0359, -0.0112, -0.0498,\n",
      "         0.0422, -0.0551,  0.0107, -0.0449,  0.0133, -0.0171, -0.0493, -0.0379,\n",
      "         0.0325,  0.0160, -0.0083, -0.0315, -0.0172, -0.0159,  0.0365, -0.0214,\n",
      "        -0.0462,  0.0145, -0.0552, -0.0095,  0.0516, -0.0570,  0.0344, -0.0081,\n",
      "        -0.0482, -0.0283, -0.0332, -0.0111,  0.0422, -0.0191,  0.0503, -0.0088,\n",
      "        -0.0511, -0.0606,  0.0291, -0.0212,  0.0372,  0.0248,  0.0199,  0.0239,\n",
      "         0.0466,  0.0063, -0.0583,  0.0396,  0.0497, -0.0264,  0.0530, -0.0152,\n",
      "         0.0112,  0.0475, -0.0462, -0.0234, -0.0312, -0.0516,  0.0112,  0.0623,\n",
      "        -0.0263,  0.0446,  0.0440,  0.0242,  0.0054,  0.0515, -0.0565, -0.0044,\n",
      "         0.0270, -0.0404, -0.0074,  0.0122, -0.0535,  0.0352, -0.0493, -0.0479,\n",
      "        -0.0208,  0.0052, -0.0294, -0.0542,  0.0523,  0.0028, -0.0078, -0.0397,\n",
      "        -0.0560,  0.0369, -0.0395,  0.0196, -0.0237, -0.0512,  0.0306,  0.0603,\n",
      "        -0.0150,  0.0455, -0.0572, -0.0553,  0.0022,  0.0342,  0.0573,  0.0053,\n",
      "         0.0367,  0.0412, -0.0462,  0.0127,  0.0144, -0.0376, -0.0133,  0.0223,\n",
      "         0.0283, -0.0322,  0.0576,  0.0072, -0.0205,  0.0076,  0.0187,  0.0139,\n",
      "        -0.0184, -0.0399, -0.0242, -0.0079,  0.0182,  0.0396, -0.0139,  0.0253,\n",
      "        -0.0563,  0.0180,  0.0221,  0.0373,  0.0355, -0.0095, -0.0321, -0.0158,\n",
      "         0.0006,  0.0506,  0.0385, -0.0503,  0.0621, -0.0171, -0.0253, -0.0026,\n",
      "         0.0010,  0.0217, -0.0568, -0.0572,  0.0221, -0.0009,  0.0468,  0.0599,\n",
      "         0.0037, -0.0609,  0.0032,  0.0259, -0.0434,  0.0206, -0.0511,  0.0242,\n",
      "        -0.0505, -0.0552, -0.0433, -0.0579,  0.0364, -0.0511,  0.0154,  0.0008,\n",
      "        -0.0109, -0.0523, -0.0612, -0.0226, -0.0100, -0.0295,  0.0237,  0.0363,\n",
      "        -0.0274,  0.0566,  0.0379,  0.0032, -0.0569,  0.0172, -0.0370, -0.0601,\n",
      "        -0.0073, -0.0587, -0.0584,  0.0164, -0.0405,  0.0614, -0.0129,  0.0010,\n",
      "         0.0341, -0.0241,  0.0085,  0.0594,  0.0189,  0.0425, -0.0437,  0.0578,\n",
      "        -0.0419, -0.0119, -0.0129, -0.0325,  0.0230, -0.0383,  0.0594,  0.0045,\n",
      "         0.0074,  0.0048,  0.0458,  0.0566, -0.0311,  0.0604, -0.0231, -0.0519])), ('putput.weight', tensor([[ 3.1864e-02,  3.8878e-02,  5.3429e-02,  2.4996e-02,  5.3467e-02,\n",
      "         -1.1922e-02, -4.8499e-03,  1.5254e-02, -3.6575e-02,  2.1766e-02,\n",
      "          1.8661e-02,  8.4016e-03,  5.2309e-02,  3.5889e-03,  1.4918e-02,\n",
      "         -1.6553e-04, -2.7390e-02, -6.1338e-02, -4.3132e-02, -3.2045e-02,\n",
      "         -1.9054e-02, -2.4385e-02,  2.0851e-02, -5.5246e-02,  2.5609e-03,\n",
      "         -3.2096e-02, -4.7402e-02, -1.1200e-02,  1.3677e-02,  3.3218e-02,\n",
      "          6.5229e-03, -1.4261e-02,  6.1026e-02, -4.0824e-02, -3.8512e-02,\n",
      "          8.2944e-03, -3.3591e-02, -3.5452e-02,  5.3000e-02, -3.9627e-02,\n",
      "          4.6900e-02, -1.8694e-02,  2.6407e-03,  1.6559e-02,  3.8756e-03,\n",
      "         -4.5898e-02, -1.2958e-02, -6.8001e-03, -2.8338e-02,  5.1643e-02,\n",
      "          6.2109e-02,  1.3932e-02, -5.6725e-03, -3.6226e-02, -4.2759e-02,\n",
      "          4.4019e-02,  1.5485e-02,  5.3043e-02, -2.4154e-03, -2.4851e-02,\n",
      "          3.7785e-02, -2.8468e-02,  4.5844e-02,  2.7034e-02, -3.7020e-02,\n",
      "         -2.1231e-02, -5.6378e-02,  3.6236e-02, -1.8364e-02, -2.7861e-02,\n",
      "         -3.4017e-03,  1.0782e-02, -5.6050e-02, -1.2408e-02, -3.8524e-03,\n",
      "          4.5233e-02,  2.6885e-02,  2.2435e-02,  4.2165e-02,  5.8582e-02,\n",
      "          6.1401e-02, -5.5202e-02, -1.8354e-02,  1.7923e-02,  4.0076e-02,\n",
      "         -2.8228e-02,  1.4581e-02, -5.6761e-03,  5.6895e-02, -6.1495e-02,\n",
      "         -9.6151e-03, -5.5527e-02,  4.3893e-02,  1.1455e-02,  1.3859e-02,\n",
      "         -5.2804e-02,  5.2696e-02, -1.0423e-02, -6.1444e-02, -4.0353e-02,\n",
      "         -3.8655e-03, -7.7468e-03,  5.4372e-02, -4.4989e-02,  5.0592e-02,\n",
      "         -8.1830e-04,  4.9773e-02, -5.8647e-02, -2.9798e-02, -5.3242e-05,\n",
      "         -5.6531e-02, -6.0596e-02, -1.0074e-02,  5.0560e-02, -4.5258e-02,\n",
      "          4.3639e-02,  4.8110e-02,  6.2251e-03,  2.3207e-02,  2.6505e-02,\n",
      "         -3.2878e-03, -4.9141e-02,  4.8242e-02,  2.4689e-02, -5.2471e-02,\n",
      "         -2.2505e-02, -2.1571e-02, -5.3941e-02, -5.2455e-02,  4.1371e-02,\n",
      "          2.8755e-02,  3.7834e-02, -3.3028e-02,  4.9158e-03, -2.8665e-02,\n",
      "          4.2864e-02, -4.0754e-03, -3.8360e-02, -4.7420e-02,  2.1412e-03,\n",
      "         -2.0772e-02,  5.5029e-02, -3.8669e-02, -4.1277e-02,  2.0425e-02,\n",
      "          1.5396e-02, -1.9697e-02, -2.7565e-02,  2.7947e-02, -5.0919e-02,\n",
      "          3.0716e-02, -4.6702e-03,  1.5560e-03,  1.8225e-02,  3.5341e-02,\n",
      "         -4.0218e-02,  5.2670e-03,  5.8832e-02,  6.2442e-02,  3.2440e-02,\n",
      "          9.7537e-03,  5.9603e-02, -5.0709e-02, -2.5551e-02,  1.8636e-02,\n",
      "          3.6603e-02,  2.4607e-03,  6.6982e-03, -1.6921e-02, -2.9057e-02,\n",
      "          1.3282e-02, -4.8274e-02, -5.0553e-03, -5.9042e-02,  7.1258e-03,\n",
      "          4.3674e-02, -5.5124e-03,  4.2669e-02, -2.5073e-02, -4.5736e-02,\n",
      "         -5.8988e-02,  4.6871e-02, -5.4755e-02,  5.1858e-02,  5.2391e-02,\n",
      "          2.2841e-02, -3.3627e-02, -1.8603e-02, -6.2306e-02, -2.2902e-02,\n",
      "          7.2008e-03, -6.2433e-02,  4.4909e-02,  3.6357e-02, -3.2849e-02,\n",
      "          5.8641e-02, -3.8317e-02, -3.0646e-02, -2.7819e-02, -2.8473e-02,\n",
      "          2.0310e-02, -3.4281e-02,  2.5127e-02, -2.5796e-03, -2.4946e-03,\n",
      "          1.7455e-02, -6.1803e-03, -9.8165e-03, -5.5219e-02, -4.8992e-02,\n",
      "          5.1488e-02, -1.8799e-03, -7.3359e-04,  2.4442e-02,  3.1072e-02,\n",
      "          4.8424e-02,  3.4845e-02, -4.7792e-03, -1.8879e-02, -3.4666e-02,\n",
      "          6.6164e-03,  1.2474e-02, -3.9450e-02, -4.0755e-02, -3.7260e-02,\n",
      "         -2.8602e-02,  4.1680e-02, -2.8814e-02, -6.1710e-02,  4.2139e-02,\n",
      "          3.1812e-02,  1.6413e-03, -4.4355e-02,  5.5831e-02,  3.2286e-02,\n",
      "         -3.5343e-02,  5.3280e-03,  3.5077e-02,  4.0732e-02,  1.0612e-02,\n",
      "          1.4255e-02,  2.0110e-02, -3.5981e-02, -4.6672e-02, -2.5789e-02,\n",
      "         -4.5870e-02, -5.1714e-02,  6.4401e-03, -9.7257e-03,  1.1389e-02,\n",
      "         -3.6875e-02, -6.1040e-02,  4.9253e-02, -6.1943e-03,  1.5841e-02,\n",
      "         -2.3529e-02]])), ('putput.bias', tensor([0.0454]))])\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "# testing forward pass\n",
    "\n",
    "\n",
    "input_tensor = torch.tensor([[-0.9994, -0.0352,  0.2698, -0.0068],\n",
    "        [-0.9947, -0.1029, -0.0644,  0.0311],\n",
    "        [-0.9969,  0.0783, -0.1867,  0.0396],\n",
    "        [-1.0000, -0.0034, -0.2585,  0.0064],\n",
    "        [-0.9993,  0.0362, -0.3160,  0.0378]])\n",
    "\n",
    "#input_tensor = torch.tensor([[-0.9994, -0.0352,  0.2698, -0.0068]])\n",
    "\n",
    "# different state dicts for Q1 and Q2\n",
    "\n",
    "#agent.critic_Q1.NN_critic(input_tensor)\n",
    "\n",
    "#print(agent.critic_Q1.NN_critic.state_dict())\n",
    "\n",
    "print(agent.critic_Q1.NN_critic(input_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christoph\\AppData\\Local\\Temp\\ipykernel_32216\\133230202.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE: TRAIN, RETURN: -1894.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1894.0737260391968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent()\n",
    "\n",
    "save_video = False\n",
    "verbose = True\n",
    "\n",
    "env = get_env(g=10.0, train=True)\n",
    "#Fails as of right now, as we need to obtain log_prob\n",
    "run_episode(env, agent, None, verbose, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.wrappers.time_limit.TimeLimit"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
