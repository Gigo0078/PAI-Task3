{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import warnings\n",
    "from typing import Union\n",
    "from utils import ReplayBuffer, get_env, run_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPISODES = 50\n",
    "TEST_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    This class implements a neural network with a variable number of hidden layers and hidden units.\n",
    "    You may use this function to parametrize your policy and critic networks.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_size: int, \n",
    "                                hidden_layers: int, activation: str):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # TODO: Implement this function which should define a neural network \n",
    "        # with a variable number of hidden layers and hidden units.\n",
    "        # Here you should define layers which your network will use.\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh()\n",
    "            }\n",
    "        self.activation = self.activations[activation]\n",
    "        self.input = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.hidden_size,self.hidden_size) for i in range(self.hidden_layers)])\n",
    "        self.putput = nn.Linear(self.hidden_size, self.output_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement the forward pass for the neural network you have defined.\n",
    "        #pass\n",
    "        s = self.input(s)\n",
    "        s = self.activation(s)\n",
    "        for i in range(0,self.hidden_layers):\n",
    "            s = self.linears[i](s)\n",
    "            s = self.activation(s)\n",
    "        s = self.putput(s)\n",
    "        s = self.activation(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,hidden_size: int, hidden_layers: int, actor_lr: float,\n",
    "                state_dim: int = 3, action_dim: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.actor_lr = actor_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.setup_actor()\n",
    "\n",
    "    def setup_actor(self):\n",
    "        '''\n",
    "        This function sets up the actor network in the Actor class.\n",
    "        '''\n",
    "        # TODO: Implement this function which sets up the actor network. \n",
    "        # Take a look at the NeuralNetwork class in utils.py. \n",
    "        #pass\n",
    "        self.NN_actor = NeuralNetwork(self.state_dim, 2*self.action_dim, self.hidden_size, self.hidden_layers, \"relu\")\n",
    "        #self.NN_actor = self.agent.trainable_params.optimizer\n",
    "\n",
    "    def clamp_log_std(self, log_std: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param log_std: torch.Tensor, log_std of the policy.\n",
    "        Returns:\n",
    "        :param log_std: torch.Tensor, log_std of the policy clamped between LOG_STD_MIN and LOG_STD_MAX.\n",
    "        '''\n",
    "        return torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "    def get_action_and_log_prob(self, state: torch.Tensor, \n",
    "                                deterministic: bool) -> (torch.Tensor, torch.Tensor):\n",
    "        '''\n",
    "        :param state: torch.Tensor, state of the agent\n",
    "        :param deterministic: boolean, if true return a deterministic action \n",
    "                                otherwise sample from the policy distribution.\n",
    "        Returns:\n",
    "        :param action: torch.Tensor, action the policy returns for the state.\n",
    "        :param log_prob: log_probability of the the action.\n",
    "        '''\n",
    "        assert state.shape == (3,) or state.shape[1] == self.state_dim, 'State passed to this method has a wrong shape'\n",
    "        action , log_prob = torch.zeros(state.shape[0]), torch.ones(state.shape[0])\n",
    "        # TODO: Implement this function which returns an action and its log probability.\n",
    "        # If working with stochastic policies, make sure that its log_std are clamped \n",
    "        # using the clamp_log_std function.\n",
    "        if deterministic == False:\n",
    "            log_std = self.clamp_log_std(log_std)\n",
    "        assert action.shape == (state.shape[0], self.action_dim) and \\\n",
    "            log_prob.shape == (state.shape[0], self.action_dim), 'Incorrect shape for action or log_prob.'\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    def __init__(self, hidden_size: int, \n",
    "                 hidden_layers: int, critic_lr: int, state_dim: int = 3, \n",
    "                    action_dim: int = 1,device: torch.device = torch.device('cpu')):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.critic_lr = critic_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.setup_critic()\n",
    "\n",
    "    def setup_critic(self):\n",
    "        # TODO: Implement this function which sets up the critic(s). Take a look at the NeuralNetwork \n",
    "        # class in utils.py. Note that you can have MULTIPLE critic networks in this class.\n",
    "        #pass\n",
    "        #We set the output to 1, but are not sure if the expected value returns a vector\n",
    "        self.NN_critic_V = NeuralNetwork(self.state_dim, 1, self.hidden_size, self.hidden_layers, \"relu\")    #---------> Still unsure what they mean by multiple critic networks\n",
    "        self.NN_critic_Q = NeuralNetwork(self.state_dim, 1, self.hidden_size, self.hidden_layers, \"relu\")\n",
    "        #self.NN_critic_lr = self.critic_lr\n",
    "        #self.optimizer = agent.trainable_params.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableParameter:\n",
    "    '''\n",
    "    This class could be used to define a trainable parameter in your method. You could find it \n",
    "    useful if you try to implement the entropy temerature parameter for SAC algorithm.\n",
    "    '''\n",
    "    def __init__(self, init_param: float, lr_param: float, \n",
    "                 train_param: bool, device: torch.device = torch.device('cpu')):\n",
    "        \n",
    "        self.log_param = torch.tensor(np.log(init_param), requires_grad=train_param, device=device)\n",
    "        self.optimizer = optim.Adam([self.log_param], lr=lr_param)\n",
    "\n",
    "    def get_param(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_param)\n",
    "\n",
    "    def get_log_param(self) -> torch.Tensor:\n",
    "        return self.log_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Environment variables. You don't need to change this.\n",
    "        self.state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "        self.action_dim = 1  # [torque] in[-1,1]\n",
    "        self.batch_size = 200\n",
    "        self.min_buffer_size = 1000\n",
    "        self.max_buffer_size = 100000\n",
    "        # If your PC possesses a GPU, you should be able to use it for training, \n",
    "        # as self.device should be 'cuda' in that case.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device: {}\".format(self.device))\n",
    "        self.memory = ReplayBuffer(self.min_buffer_size, self.max_buffer_size, self.device)\n",
    "        \n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        # TODO: Setup off-policy agent with policy and critic classes. \n",
    "        # Feel free to instantiate any other parameters you feel you might need.   \n",
    "        #pass\n",
    "        self.hidden_layers = 2\n",
    "        self.hidden_size = 256\n",
    "        self.lr = 3E-4\n",
    "        self.actor = Actor(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        self.critic = Critic(self.hidden_size, self.hidden_layers, self.lr)\n",
    "        #self.trainable_params = TrainableParameter(init_param: float, self.lr, train_param: bool)\n",
    "        #Name parameters from the paper\n",
    "        #self.log_prob = []\n",
    "        self.Tau = 0.005\n",
    "\n",
    "    def get_action(self, s: np.ndarray, train: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param s: np.ndarray, state of the pendulum. shape (3, )\n",
    "        :param train: boolean to indicate if you are in eval or train mode. \n",
    "                    You can find it useful if you want to sample from deterministic policy.\n",
    "        :return: np.ndarray,, action to apply on the environment, shape (1,)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a function that returns an action from the policy for the state s.\n",
    "        #action = np.random.uniform(-1, 1, (1,))\n",
    "        #Convert the state to a torch tensor, which is the required input for the actor\n",
    "        s = torch.tensor(s)\n",
    "        #Import action from the actor and discard the log probability here, possibly used elsewhere\n",
    "        action, _ = self.actor.get_action_and_log_prob(s, not(train))\n",
    "        # only get one action -> we have to sample in get_action_and_log_prob\n",
    "        #Convert the returned tensor action to an nd.array\n",
    "        action = action.numpy()\n",
    "        #Need log probability for something -------> ?\n",
    "\n",
    "        assert action.shape == (1,), 'Incorrect action shape.'\n",
    "        assert isinstance(action, np.ndarray ), 'Action dtype must be np.ndarray' \n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    def run_gradient_update_step(object: Union[Actor, Critic], loss: torch.Tensor):\n",
    "        '''\n",
    "        This function takes in a object containing trainable parameters and an optimizer, \n",
    "        and using a given loss, runs one step of gradient update. If you set up trainable parameters \n",
    "        and optimizer inside the object, you could find this function useful while training.\n",
    "        :param object: object containing trainable parameters and an optimizer\n",
    "        '''\n",
    "        object.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        object.optimizer.step()\n",
    "\n",
    "    def critic_target_update(self, base_net: NeuralNetwork, target_net: NeuralNetwork, \n",
    "                             tau: float, soft_update: bool):\n",
    "        '''\n",
    "        This method updates the target network parameters using the source network parameters.\n",
    "        If soft_update is True, then perform a soft update, otherwise a hard update (copy).\n",
    "        :param base_net: source network\n",
    "        :param target_net: target network\n",
    "        :param tau: soft update parameter\n",
    "        :param soft_update: boolean to indicate whether to perform a soft update or not\n",
    "        '''\n",
    "        for param_target, param in zip(target_net.parameters(), base_net.parameters()):\n",
    "            if soft_update:\n",
    "                param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "            else:\n",
    "                param_target.data.copy_(param.data)\n",
    "\n",
    "    def train_agent(self):  #------------> ? Christoph is a bit confused, but we need to implement phi, psi and theta gradient updates\n",
    "        '''\n",
    "        This function represents one training iteration for the agent. It samples a batch \n",
    "        from the replay buffer,and then updates the policy and critic networks \n",
    "        using the sampled batch.\n",
    "        '''\n",
    "        # TODO: Implement one step of training for the agent.\n",
    "        # Hint: You can use the run_gradient_update_step for each policy and critic.\n",
    "        # Example: self.run_gradient_update_step(self.policy, policy_loss)\n",
    "\n",
    "        # Batch sampling\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch = batch\n",
    "\n",
    "        # TODO: Implement Critic(s) update here.\n",
    "        #Since we're performing an update step, we must include this new sampled batch in the neural network\n",
    "        #reward_sampled = -(s_batch^2 + 0.1*s_prime_batch^2 + 0.001*a_batch^2)\n",
    "        self.actor.NN_actor.train() #Train the actor\n",
    "        self.critic.NN_critic_V.train() #Train the first critic\n",
    "        self.critic.NN_critic_Q.train() #Train the second critic\n",
    "\n",
    "        #Define their loss functions\n",
    "        loss_actor = 0\n",
    "        loss_critic_V = 0\n",
    "        loss_critic_Q = 0\n",
    "\n",
    "        # TODO: Implement Policy update here\n",
    "        #policy = np.transpose(np.array([np.cos(s_batch),np.sin(s_batch),s_prime_batch]))\n",
    "\n",
    "        #For sure we will have to perform a gradient update step here \n",
    "        self.run_gradient_update_step(self.actor, loss_actor)  #----------> Not sure what loss to use\n",
    "\n",
    "        #Hmm still unsure about how we will use the critic target update, also need to figure out what the relevant parameters for this are\n",
    "        self.run_gradient_update_step(self.critic, loss_critic_V)\n",
    "        self.run_gradient_update_step(self.critic, loss_critic_Q)\n",
    "        #Unsure here, what the input and what the output networks are\n",
    "        self.critic_target_update(self.actor.NN_actor,self.critic.NN_critic,self.Tau,True)  #--------> Need to find out when to perform a soft and hard update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "#Env determines the Markov Decision Process\n",
    "env = get_env(g=10.0, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'log_std' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oliwi\\Documents\\PAI\\Task4\\PAI-Task4\\solution.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m save_video \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m verbose \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m run_episode(env, agent, \u001b[39mNone\u001b[39;49;00m, verbose, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\oliwi\\Documents\\PAI\\Task4\\PAI-Task4\\utils.py:113\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(env, agent, rec, verbose, train)\u001b[0m\n\u001b[0;32m    111\u001b[0m episode_return, truncated \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated:\n\u001b[1;32m--> 113\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state, train)\n\u001b[0;32m    115\u001b[0m     state_prime, reward, _, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m train:\n",
      "\u001b[1;32mc:\\Users\\oliwi\\Documents\\PAI\\Task4\\PAI-Task4\\solution.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m s \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(s)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m#Import action from the actor and discard the log probability here, possibly used elsewhere\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mget_action_and_log_prob(s, \u001b[39mnot\u001b[39;49;00m(train))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# only get one action -> we have to sample in get_action_and_log_prob\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m#Convert the returned tensor action to an nd.array\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32mc:\\Users\\oliwi\\Documents\\PAI\\Task4\\PAI-Task4\\solution.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# TODO: Implement this function which returns an action and its log probability.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# If working with stochastic policies, make sure that its log_std are clamped \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# using the clamp_log_std function.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     log_std \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclamp_log_std(log_std)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39massert\u001b[39;00m action\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (state\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dim) \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     log_prob\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (state\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dim), \u001b[39m'\u001b[39m\u001b[39mIncorrect shape for action or log_prob.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/oliwi/Documents/PAI/Task4/PAI-Task4/solution.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action, log_prob\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'log_std' referenced before assignment"
     ]
    }
   ],
   "source": [
    "save_video = False\n",
    "verbose = True\n",
    "#Fails as of right now, as we need to obtain log_prob\n",
    "run_episode(env, agent, None, verbose, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.wrappers.time_limit.TimeLimit"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Task4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
