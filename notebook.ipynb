{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import warnings\n",
    "from typing import Union\n",
    "from utils import ReplayBuffer, get_env, run_episode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    '''\n",
    "    This class implements a neural network with a variable number of hidden layers and hidden units.\n",
    "    You may use this function to parametrize your policy and critic networks.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_size: int, \n",
    "                                hidden_layers: int, activation: str):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # TODO: Implement this function which should define a neural network \n",
    "        # with a variable number of hidden layers and hidden units.\n",
    "        # Here you should define layers which your network will use.\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activations = {\n",
    "            'relu': nn.ReLU(),\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh()\n",
    "            }\n",
    "        self.activation = self.activations[activation]\n",
    "        self.input = nn.Linear(self.input_dim, self.hidden_size)\n",
    "        self.linears = nn.ModuleList([nn.Linear(self.hidden_size,self.hidden_size) for i in range(self.hidden_layers)])\n",
    "        self.putput = nn.Linear(self.hidden_size, self.output_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: Implement the forward pass for the neural network you have defined.\n",
    "        #pass\n",
    "        s = self.input(s)\n",
    "        s = self.activation(s)\n",
    "        for i in range(0,self.hidden_layers):\n",
    "            s = self.linears[i](s)\n",
    "            s = self.activation(s)\n",
    "        s = self.putput(s)\n",
    "        s = self.activation(s)\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self,hidden_size: int, hidden_layers: int, actor_lr: float,\n",
    "                state_dim: int = 3, action_dim: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.actor_lr = actor_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.setup_actor()\n",
    "\n",
    "    def setup_actor(self):\n",
    "        '''\n",
    "        This function sets up the actor network in the Actor class.\n",
    "        '''\n",
    "        # TODO: Implement this function which sets up the actor network. \n",
    "        # Take a look at the NeuralNetwork class in utils.py. \n",
    "        \n",
    "        self.NN_actor = NeuralNetwork(self.state_dim, \n",
    "                                      self.action_dim + 1, \n",
    "                                      self.hidden_size, \n",
    "                                      self.hidden_layers, \"relu\")\n",
    "        \n",
    "        # ---- other parameters for training ------\n",
    "\n",
    "        self.optimizer = optim.Adam(self.NN_actor.parameters(), lr=self.actor_lr)\n",
    "\n",
    "    def clamp_log_std(self, log_std: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        :param log_std: torch.Tensor, log_std of the policy.\n",
    "        Returns:\n",
    "        :param log_std: torch.Tensor, log_std of the policy clamped between LOG_STD_MIN and LOG_STD_MAX.\n",
    "        '''\n",
    "        return torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "\n",
    "    def get_action_and_log_prob(self, state: torch.Tensor, \n",
    "                                deterministic: bool) -> (torch.Tensor, torch.Tensor):\n",
    "        '''\n",
    "        :param state: torch.Tensor, state of the agent\n",
    "        :param deterministic: boolean, if true return a deterministic action \n",
    "                                otherwise sample from the policy distribution.\n",
    "        Returns:\n",
    "        :param action: torch.Tensor, action the policy returns for the state.\n",
    "        :param log_prob: log_probability of the the action.\n",
    "        '''\n",
    "        assert state.shape == (3,) or state.shape[1] == self.state_dim, 'State passed to this method has a wrong shape'\n",
    "        action , log_prob = torch.zeros(state.shape[0], 1), torch.ones(state.shape[0], 1)\n",
    "        # TODO: Implement this function which returns an action and its log probability.\n",
    "        # If working with stochastic policies, make sure that its log_std are clamped \n",
    "        # using the clamp_log_std function.\n",
    "        #if deterministic == False:\n",
    "        #log_std = self.clamp_log_std(log_std)\n",
    "\n",
    "        print(\"shape\", action.shape)\n",
    "\n",
    "        assert action.shape == (state.shape[0], self.action_dim) and \\\n",
    "            log_prob.shape == (state.shape[0], self.action_dim), 'Incorrect shape for action or log_prob.'\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, hidden_size: int, \n",
    "                 hidden_layers: int, critic_lr: int, state_dim: int = 3, \n",
    "                    action_dim: int = 1,device: torch.device = torch.device('cpu')):\n",
    "        super(Critic, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.critic_lr = critic_lr\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.setup_critic()\n",
    "\n",
    "    def setup_critic(self):\n",
    "        # TODO: Implement this function which sets up the critic(s). Take a look at the NeuralNetwork \n",
    "        # class in utils.py. Note that you can have MULTIPLE critic networks in this class.\n",
    "        #pass\n",
    "        #We set the output to 1, but are not sure if the expected value returns a vector\n",
    "        self.NN_critic_V = NeuralNetwork(self.state_dim, 1, self.hidden_size, self.hidden_layers, \"relu\")    #---------> Still unsure what they mean by multiple critic networks\n",
    "\n",
    "        # ---- other parameters for training ------\n",
    "\n",
    "        self.optimizer = optim.Adam(self.NN_critic_V.parameters(), lr=self.critic_lr)\n",
    "\n",
    "        # alpha parameter in the SAC paper\n",
    "        self.temperature = TrainableParameter(init_param = 0.005, \n",
    "                                              lr_param = 0.1,\n",
    "                                              train_param = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network <generator object Module.parameters at 0x123d47b50>\n",
      "Temperature <__main__.TrainableParameter object at 0x123de1c90>\n",
      "input dim is Linear(in_features=3, out_features=25, bias=True)\n",
      "tensor(0.0050, dtype=torch.float64, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "C = Critic(hidden_size = 25, \n",
    "            hidden_layers = 3,\n",
    "            critic_lr = 0.1)\n",
    "\n",
    "print(\"Neural network\", C.NN_critic_V.parameters()) # network parameters\n",
    "print(\"Temperature\", C.temperature)\n",
    "\n",
    "print(\"input dim is\", C.NN_critic_V.input)\n",
    "\n",
    "input = torch.tensor([1.,2.,2.]) # convert inputs to float (important)\n",
    "\n",
    "C.temperature.get_param() * C.NN_critic_V.forward(input)\n",
    "\n",
    "print(C.temperature.get_param()) # part of a computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable Parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableParameter:\n",
    "    '''\n",
    "    This class could be used to define a trainable parameter in your method. You could find it \n",
    "    useful if you try to implement the entropy temerature parameter for SAC algorithm.\n",
    "    '''\n",
    "    def __init__(self, init_param: float, lr_param: float, \n",
    "                 train_param: bool, device: torch.device = torch.device('cpu')):\n",
    "        \n",
    "        self.log_param = torch.tensor(np.log(init_param), requires_grad=train_param, device=device)\n",
    "        self.optimizer = optim.Adam([self.log_param], lr=lr_param)\n",
    "\n",
    "    def get_param(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_param)\n",
    "\n",
    "    def get_log_param(self) -> torch.Tensor:\n",
    "        return self.log_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        # Environment variables. You don't need to change this.\n",
    "        self.state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "        self.action_dim = 1  # [torque] in[-1,1]\n",
    "        self.batch_size = 200 # each batch is an episode\n",
    "        self.min_buffer_size = 1000\n",
    "        self.max_buffer_size = 100000\n",
    "        # If your PC possesses a GPU, you should be able to use it for training, \n",
    "        # as self.device should be 'cuda' in that case.\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device: {}\".format(self.device))\n",
    "        self.memory = ReplayBuffer(self.min_buffer_size, self.max_buffer_size, self.device)\n",
    "        \n",
    "        self.setup_agent()\n",
    "\n",
    "    def setup_agent(self):\n",
    "        # TODO: Setup off-policy agent with policy and critic classes. \n",
    "        # Feel free to instantiate any other parameters you feel you might need.   \n",
    "\n",
    "        self.actor = Actor(hidden_size = 25, \n",
    "                             hidden_layers = 3,\n",
    "                             actor_lr = 0.1)\n",
    "        \n",
    "        self.critic = Critic(hidden_size = 25, \n",
    "                             hidden_layers = 3,\n",
    "                             critic_lr = 0.1)\n",
    "\n",
    "        #Name parameters from the paper\n",
    "\n",
    "        self.Tau = 0.005\n",
    "\n",
    "    def get_action(self, s: np.ndarray, train: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param s: np.ndarray, state of the pendulum. shape (3, )\n",
    "        :param train: boolean to indicate if you are in eval or train mode. \n",
    "                    You can find it useful if you want to sample from deterministic policy.\n",
    "        :return: np.ndarray,, action to apply on the environment, shape (1,)\n",
    "        \"\"\"\n",
    "        # TODO: Implement a function that returns an action from the policy for the state s.\n",
    "        #action = np.random.uniform(-1, 1, (1,))\n",
    "        # Convert the state to a torch tensor, which is the required input for the actor\n",
    "        s = torch.tensor(s)\n",
    "        #Import action from the actor and discard the log probability here, possibly used elsewhere\n",
    "        action, _ = self.actor.get_action_and_log_prob(s, not(train))\n",
    "        # only get one action -> we have to sample in get_action_and_log_prob\n",
    "        #Convert the returned tensor action to an nd.array\n",
    "        action = action[0].numpy()\n",
    "        #Need log probability for something -------> ?\n",
    "\n",
    "        assert action.shape == (1,), 'Incorrect action shape.'\n",
    "        assert isinstance(action, np.ndarray ), 'Action dtype must be np.ndarray' \n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    # Union[Actor, Critic] means that object is either Actor or Critic\n",
    "    def run_gradient_update_step(object: Union[Actor, Critic], loss: torch.Tensor):\n",
    "        '''\n",
    "        This function takes in a object containing trainable parameters and an optimizer, \n",
    "        and using a given loss, runs one step of gradient update. If you set up trainable parameters \n",
    "        and optimizer inside the object, you could find this function useful while training.\n",
    "        :param object: object containing trainable parameters and an optimizer\n",
    "        '''\n",
    "        object.optimizer.zero_grad() \n",
    "        loss.mean().backward() \n",
    "        object.optimizer.step() \n",
    "\n",
    "        # what does this return? nothing of interest\n",
    "        # performs update of network inside of the object\n",
    "        # also updates trainable parameters (temperature for critic network)\n",
    "\n",
    "    def critic_target_update(self, base_net: NeuralNetwork, target_net: NeuralNetwork, \n",
    "                             tau: float, soft_update: bool):\n",
    "        '''\n",
    "        This method updates the target network parameters using the source network parameters.\n",
    "        If soft_update is True, then perform a soft update, otherwise a hard update (copy).\n",
    "        :param base_net: source network\n",
    "        :param target_net: target network\n",
    "        :param tau: soft update parameter\n",
    "        :param soft_update: boolean to indicate whether to perform a soft update or not\n",
    "        '''\n",
    "        for param_target, param in zip(target_net.parameters(), base_net.parameters()):\n",
    "            if soft_update:\n",
    "                param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
    "            else:\n",
    "                param_target.data.copy_(param.data)\n",
    "\n",
    "    def train_agent(self):  #------------> ? Christoph is a bit confused, but we need to implement phi, psi and theta gradient updates\n",
    "        '''\n",
    "        This function represents one training iteration for the agent. It samples a batch \n",
    "        from the replay buffer,and then updates the policy and critic networks \n",
    "        using the sampled batch.\n",
    "        '''\n",
    "        # TODO: Implement one step of training for the agent.\n",
    "        # Hint: You can use the run_gradient_update_step for each policy and critic.\n",
    "        # Example: self.run_gradient_update_step(self.policy, policy_loss)\n",
    "\n",
    "        # Batch sampling\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        s_batch, a_batch, r_batch, s_prime_batch = batch\n",
    "\n",
    "        print(\"size of batch\", batch.size)\n",
    "        \n",
    "        # ---- TODO: Implement Critic(s) update here.\n",
    "\n",
    "        # compute critic loss given the batch (is this a float?) => J_Q value\n",
    "        temp = self.critic.temperature.get_param()\n",
    "        print(\"temperature is\", temp)\n",
    "        value = 0. + temp * 0. # probs with a loop\n",
    "        critic_loss = torch.tensor(value)\n",
    "\n",
    "        # store current network (used for bootstrap estimate\n",
    "        base_net = self.critic.NN_critic_V\n",
    "\n",
    "        # perform update step  \n",
    "        self.run_gradient_update_step(self.critic, critic_loss)\n",
    "\n",
    "        print(\"updated parameter temperature is\", self.self.critic.temperature.get_param())\n",
    "\n",
    "        self.critic_target_update(base_net = base_net, \n",
    "                                 target_net = self.critic.NN_critic_V, \n",
    "                                 tau = self.Tau, soft_update =  True)\n",
    "\n",
    "\n",
    "        # ----- TODO: Implement Policy update here\n",
    "\n",
    "        # compute policy loss given the batch, J_pi value\n",
    "        policy_loss = 0.123\n",
    "\n",
    "        # perform update step  \n",
    "        self.run_gradient_update_step(self.actor, policy_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "shape torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Agent()\n",
    "\n",
    "s = np.array([1., 2., 3.])\n",
    "\n",
    "a.get_action(s, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EPISODES = 50\n",
    "TEST_EPISODES = 300\n",
    "\n",
    "# You may set the save_video param to output the video of one of the evalution episodes, or \n",
    "# you can disable console printing during training and testing by setting verbose to False.\n",
    "save_video = False\n",
    "verbose = True\n",
    "\n",
    "agent = Agent()\n",
    "env = get_env(g=10.0, train=True)\n",
    "\n",
    "for EP in range(TRAIN_EPISODES):\n",
    "    run_episode(env, agent, None, verbose, train=True)\n",
    "\n",
    "if verbose:\n",
    "    print('\\n')\n",
    "\n",
    "test_returns = []\n",
    "env = get_env(g=10.0, train=False)\n",
    "\n",
    "if save_video:\n",
    "    video_rec = VideoRecorder(env, \"pendulum_episode.mp4\")\n",
    "\n",
    "for EP in range(TEST_EPISODES):\n",
    "    rec = video_rec if (save_video and EP == TEST_EPISODES - 1) else None\n",
    "    with torch.no_grad():\n",
    "        episode_return = run_episode(env, agent, rec, verbose, train=False)\n",
    "    test_returns.append(episode_return)\n",
    "\n",
    "avg_test_return = np.mean(np.array(test_returns))\n",
    "\n",
    "print(\"\\n AVG_TEST_RETURN:{:.1f} \\n\".format(avg_test_return))\n",
    "\n",
    "if save_video:\n",
    "    video_rec.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
